1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 1
Discovery of Ranking Fraud for Mobile Apps
Hengshu Zhu, Hui Xiong, Senior Member, IEEE, Yong Ge, and Enhong Chen, Senior Member, IEEE
Abstract—Ranking fraud in the mobile App market refers to fraudulent or deceptive activities which have a purpose of bumping up
the Apps in the popularity list. Indeed, it becomes more and more frequent for App developers to use shady means, such as inflating
their Apps’ sales or posting phony App ratings, to commit ranking fraud. While the importance of preventing ranking fraud has been
widely recognized, there is limited understanding and research in this area. To this end, in this paper, we provide a holistic view of
ranking fraud and propose a ranking fraud detection system for mobile Apps. Specifically, we first propose to accurately locate the
ranking fraud by mining the active periods, namely leading sessions, of mobile Apps. Such leading sessions can be leveraged for
detecting the local anomaly instead of global anomaly of App rankings. Furthermore, we investigate three types of evidences, i.e.,
ranking based evidences, rating based evidences and review based evidences, by modeling Apps’ ranking, rating and review behaviors
through statistical hypotheses tests. In addition, we propose an optimization based aggregation method to integrate all the evidences
for fraud detection. Finally, we evaluate the proposed system with real-world App data collected from the iOS App Store for a long time
period. In the experiments, we validate the effectiveness of the proposed system, and show the scalability of the detection algorithm as
well as some regularity of ranking fraud activities.
Index Terms—Mobile Apps, Ranking Fraud Detection, Evidence Aggregation, Historical Ranking Records, Rating and Review.
F
1 INTRODUCTION
The number of mobile Apps has grown at a breathtaking
rate over the past few years. For example, as of the end
of April 2013, there are more than 1.6 million Apps at
Apple’s App store and Google Play. To stimulate the
development of mobile Apps, many App stores launched
daily App leaderboards, which demonstrate the chart
rankings of most popular Apps. Indeed, the App leader-
board is one of the most important ways for promoting
mobile Apps. A higher rank on the leaderboard usually
leads to a huge number of downloads and million
dollars in revenue. Therefore, App developers tend to
explore various ways such as advertising campaigns to
promote their Apps in order to have their Apps ranked
as high as possible in such App leaderboards.
However, as a recent trend, instead of relying on
traditional marketing solutions, shady App developers
resort to some fraudulent means to deliberately boost
their Apps and eventually manipulate the chart rankings
on an App store. This is usually implemented by using
so-called “bot farms” or “human water armies” to inflate
the App downloads, ratings and reviews in a very
 H. Zhu and E. Chen are with the School of Computer Science and
Technology, University of Science and Technology of China, Hefei, Anhui
230026, China.
Email: zhs@mail.ustc.edu.cn; cheneh@ustc.edu.cn
 H. Xiong is with the Management Science and Information Systems
Department, Rutgers Business School, Rutgers University, Newark, NJ
07102 USA.
Email: hxiong@rutgers.edu
 Y. Ge is with the Computer Science Department, UNC Charlotte, Char-
lotte, NC 28223 USA.
Email: yong.ge@uncc.edu
This is a substantially extended and revised version of [33], which appears
in Proceedings of the 22nd ACM Conference on Information and Knowledge
Management (CIKM2013).
short time. For example, an article from VentureBeat [4]
reported that, when an App was promoted with the
help of ranking manipulation, it could be propelled
from number 1,800 to the top 25 in Apple’s top free
leaderboard and more than 50,000-100,000 new users
could be acquired within a couple of days. In fact, such
ranking fraud raises great concerns to the mobile App
industry. For example, Apple has warned of cracking
down on App developers who commit ranking fraud [3]
in the Apple’s App store.
In the literature, while there are some related work,
such as web ranking spam detection [22], [25], [30],
online review spam detection [19], [27], [28], and mobile
App recommendation [24], [29], [31], [32], the problem of
detecting ranking fraud for mobile Apps is still under-
explored. To fill this crucial void, in this paper, we
propose to develop a ranking fraud detection system for
mobile Apps. Along this line, we identify several im-
portant challenges. First, ranking fraud does not always
happen in the whole life cycle of an App, so we need
to detect the time when fraud happens. Such challenge
can be regarded as detecting the local anomaly instead of
global anomaly of mobile Apps. Second, due to the huge
number of mobile Apps, it is difficult to manually label
ranking fraud for each App, so it is important to have
a scalable way to automatically detect ranking fraud
without using any benchmark information. Finally, due
to the dynamic nature of chart rankings, it is not easy
to identify and confirm the evidences linked to ranking
fraud, which motivates us to discover some implicit
fraud patterns of mobile Apps as evidences.
Indeed, our careful observation reveals that mobile
Apps are not always ranked high in the leaderboard, but
only in some leading events, which form different leading
sessions. Note that we will introduce both leading events
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 2
MINING LEADING 
SESSIONS
RANKING BASED 
EVIDENCES
RATING BASED 
EVIDENCES
EVIDENCE 
AGGREGATION
START
INPUT
Mobile Apps          
OUTPUT
END
Work Flow
Data Flow
REVIEW BASED 
EVIDENCES
Historical Records          
Fig. 1. The framework of our ranking fraud detection
system for mobile Apps.
and leading sessions in detail later. In other words,
ranking fraud usually happens in these leading sessions.
Therefore, detecting ranking fraud of mobile Apps is
actually to detect ranking fraud within leading sessions
of mobile Apps. Specifically, we first propose a simple
yet effective algorithm to identify the leading sessions of
each App based on its historical ranking records. Then,
with the analysis of Apps’ ranking behaviors, we find
that the fraudulent Apps often have different ranking
patterns in each leading session compared with nor-
mal Apps. Thus, we characterize some fraud evidences
from Apps’ historical ranking records, and develop three
functions to extract such ranking based fraud evidences.
Nonetheless, the ranking based evidences can be affected
by App developers’ reputation and some legitimate mar-
keting campaigns, such as “limited-time discount”. As
a result, it is not sufficient to only use ranking based
evidences. Therefore, we further propose two types of
fraud evidences based on Apps’ rating and review his-
tory, which reflect some anomaly patterns from Apps’
historical rating and review records. In addition, we
develop an unsupervised evidence-aggregation method
to integrate these three types of evidences for evaluating
the credibility of leading sessions from mobile Apps.
Figure 1 shows the framework of our ranking fraud
detection system for mobile Apps.
It is worth noting that all the evidences are extracted
by modeling Apps’ ranking, rating and review behaviors
through statistical hypotheses tests. The proposed frame-
work is scalable and can be extended with other domain-
generated evidences for ranking fraud detection. Finally,
we evaluate the proposed system with real-world App
data collected from the Apple’s App store for a long
time period, i.e., more than two years. Experimental
results show the effectiveness of the proposed system,
the scalability of the detection algorithm as well as some
regularity of ranking fraud activities.
Overview. The remainder of this paper is organized as
follows. In Section 2, we introduce some preliminaries
and how to mine leading sessions for mobile Apps.
Section 3 presents how to extract ranking, rating and
review based evidences and combine them for ranking
fraud detection. In Section 4 we make some further
discussion about the proposed approach. In Section 5,
we report the experimental results on two long-term
real-world data sets. Section 6 provides a brief review
of related works. Finally, in Section 7, we conclude the
paper and propose some future research directions.
2 IDENTIFYING LEADING SESSIONS FOR MO-
BILE APPS
In this section, we first introduce some preliminaries,
and then show how to mine leading sessions for mobile
Apps from their historical ranking records.
2.1 Preliminaries
The App leaderboard demonstrates top K popular Apps
with respect to different categories, such as “Top Free
Apps” and “Top Paid Apps”. Moreover, the leaderboard
is usually updated periodically (e.g., daily). There-
fore, each mobile App a has many historical ranking
records which can be denoted as a time series, Ra =
fra1 ;    ; rai ;    ; rang, where rai 2 f1; :::;K;+1g is the
ranking of a at time stamp ti; +1 means a is not ranked
in the top K list; n denotes the number of all ranking
records. Note that, the smaller value rai has, the higher
ranking position the App obtains.
By analyzing the historical ranking records of mobile
Apps, we observe that Apps are not always ranked
high in the leaderboard, but only in some leading events.
For example, Figure 2 (a) shows an example of leading
events of a mobile App. Formally, we define a leading
event as follows.
Definition 1 (Leading Event): Given a ranking thresh-
old K 2 [1;K], a leading event e of App a contains
a time range Te = [testart; teend] and corresponding rank-
ings of a, which satisfies rastart  K < rastart 1, and
raend  K < raend+1. Moreover, 8tk 2 (testart; teend), we
have rak  K.
Note that we apply a ranking threshold K which is
usually smaller than K here because K may be very big
(e.g., more than 1000), and the ranking records beyond
K (e.g., 300) are not very useful for detecting the
ranking manipulations.
Furthermore, we also find that some Apps have sev-
eral adjacent leading events which are close to each
other and form a leading session. For example, Figure 2(b)
shows an example of adjacent leading events of a given
mobile App, which form two leading sessions. Particu-
larly, a leading event which does not have other nearby
neighbors can also be treated as a special leading session.
The formal definition of leading session is as follows.
Definition 2 (Leading Session): A leading session s of
App a contains a time range Ts = [tsstart; tsend] and
n adjacent leading events fe1; :::; eng, which satisfies
tsstart = t
e1
start, tsend = t
en
end and there is no other leading
session s that makes Ts  Ts . Meanwhile, 8i 2 [1; n),
we have (tei+1start  teiend) < , where  is a predefined time
threshold for merging leading events.
Intuitively, the leading sessions of a mobile App rep-
resent its periods of popularity, so the ranking manip-
ulation will only take place in these leading sessions.
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 3
Event 2
Event 1
(a) Leading Events
 Session 1 Session 2
(b) Leading Sessions
Fig. 2. (a) Example of leading events; (b) Example of
leading sessions of mobile Apps.
Therefore, the problem of detecting ranking fraud is to
detect fraudulent leading sessions. Along this line, the
first task is how to mine the leading sessions of a mobile
App from its historical ranking records.
2.2 Mining Leading Sessions
There are two main steps for mining leading sessions.
First, we need to discover leading events from the App’s
historical ranking records. Second, we need to merge
adjacent leading events for constructing leading sessions.
Specifically, Algorithm 1 demonstrates the pseudo code
of mining leading sessions for a given App a.
Algorithm 1 Mining Leading Sessions
Input 1: a’s historical ranking records Ra;
Input 2: the ranking threshold K;
Input 2: the merging threshold ;
Output: the set of a’s leading sessions Sa;
Initialization: Sa = ;;
1: Es = ;; e = ;; s = ;; testart = 0;
2: for each i 2 [1; jRaj] do
3: if rai  K and testart == 0 then
4: testart = ti;
5: else if rai > K and testart 6= 0 then
6: //found one event;
7: teend = ti 1; e =< t
e
start; t
e
end >;
8: if Es == ; then
9: Es[ = e; tsstart = testart; tsend = teend;
10: else if (testart   tsend) <  then
11: Es[ = e; tsend = teend;
12: else then
13: //found one session;
14: s =< tsstart; t
s
end; Es >;
15: Sa[ = s; s = ; is a new session;
16: Es = feg; tsstart = testart; tsend = teend;
17: testart = 0; e = ; is a new leading event;
18: return Sa
In Algorithm 1, we denote each leading event e and
session s as tuples < testart; teend > and < t
s
start; t
s
end; Es >
respectively, where Es is the set of leading events in
session s. Specifically, we first extract individual leading
event e for the given App a (i.e., Step 2 to 7) from the
beginning time. For each extracted individual leading
event e, we check the time span between e and the
current leading session s to decide whether they belong
to the same leading session based on Definition 2. Partic-
ularly, if (testart tsend) < , e will be considered as a new
leading session (i.e., Step 8 to 16). Thus, this algorithm
can identify leading events and sessions by scanning a’s
historical ranking records only once.
Fig. 3. An example of different ranking phases of a
leading event.
3 EXTRACTING EVIDENCES FOR RANKING
FRAUD DETECTION
In this section, we study how to extract and combine
fraud evidences for ranking fraud detection.
3.1 Ranking based Evidences
According to the definitions introduced in Section 2, a
leading session is composed of several leading events.
Therefore, we should first analyze the basic characteris-
tics of leading events for extracting fraud evidences.
By analyzing the Apps’ historical ranking records,
we observe that Apps’ ranking behaviors in a leading
event always satisfy a specific ranking pattern, which
consists of three different ranking phases, namely, rising
phase, maintaining phase and recession phase. Specifically, in
each leading event, an App’s ranking first increases to a
peak position in the leaderboard (i.e., rising phase), then
keeps such peak position for a period (i.e., maintaining
phase), and finally decreases till the end of the event
(i.e., recession phase). Figure 3 shows an example of
different ranking phases of a leading event. Indeed, such
a ranking pattern shows an important understanding of
leading event. In the following, we formally define the
three ranking phases of a leading event.
Definition 3 (Ranking Phases of a Leading Event):
Given a leading event e of App a with time range
[testart; t
e
end], where the highest ranking position of a
is rapeak, which belongs to R. The rising phase of
e is a time range [tea; teb], where t
e
a = t
e
start, rab 2 R
and 8ti 2 [tea; teb) satisfies rai 62 R. The maintaining
phase of e is a time range [teb; t
e
c], where rac 2 R and
8ti 2 (tec; teend] satisfies rai 62 R. The recession phase is
a time range [tec; ted], where t
e
d = t
e
end.
Note that, in Definition 3, R is a ranking range
to decide the beginning time and the end time of the
maintaining phase. teb and t
e
c are the first and last time
when the App is ranked into R. It is because an
App, even with ranking manipulation, cannot always
maintain the same peak position (e.g., rank 1) in the
leaderboard but only in a ranking range (e.g., top 25).
If a leading session s of App a has ranking fraud,
a’s ranking behaviors in these three ranking phases of
leading events in s should be different from those in
a normal leading session. Actually, we find that each
App with ranking manipulation always has an expected
ranking target (e.g., top 25 in leaderboard for one week)
and the hired marketing firms also charge money accord-
ing to such ranking expectation (e.g., $1000/day in top
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 4
(a) Example 1 (b) Example 2
Fig. 4. Two real-world examples of leading events.
25). Therefore, for both App developers and marketing
firms, the earlier the ranking expectation meets, the more
money can be earned. Moreover, after reaching and
maintaining the expected ranking for a required period,
the manipulation will be stopped and the ranking of the
malicious App will decrease dramatically. As a result,
the suspicious leading events may contain very short
rising and recession phases. Meanwhile, the cost of
ranking manipulation with high ranking expectations is
quite expensive due to the unclear ranking principles
of App stores and the fierce competition between App
developers. Therefore, the leading event of fraudulent
Apps often has very short maintaining phase with high
ranking positions.
Figure 4 (a) shows an example of ranking records from
one of the reported suspicious Apps [5]. We can see that
this App has several impulsive leading events with high
ranking positions. In contrast, the ranking behaviors of a
normal App’s leading event may be completely different.
For example, Figure 4 (b) shows an example of ranking
records from a popular App “Angry Birds: Space”, which
contains a leading event with a long time range (i.e.,
more than one year), especially for the recession phase.
In fact, once a normal App is ranked high in the leader-
board, it often owns lots of honest fans and may attract
more and more users to download. Therefore, this App
will be ranked high in the leaderboard for a long time.
Based on the above discussion, we propose some ranking
based signatures of leading sessions to construct fraud
evidences for ranking fraud detection.
 EVIDENCE 1. As shown in Figure 3, we use two
shape parameters 1 and 2 to quantify the ranking
patterns of the rising phase and the recession phase of
App a’s leading event e, which can be computed by
e1 = arctan(
K   rab
teb   tea
); e2 = arctan(
K   rac
ted   tec
): (1)
where K is the ranking threshold in Definition 1. In-
tuitively, a large 1 may indicate that the App has been
bumped to a high rank within a short time, and a large 2
may indicate that the App has dropped from a high rank
to the bottom within a short time. Therefore, a leading
session, which has more leading events with large 1
and 2 values, has higher probability of having ranking
fraud. Here, we define a fraud signature s for a leading
session as follows.
s =
1
jEsj
X
e2s
(e1 + 
e
2); (2)
where jEsj is the number of leading events in session
s. Intuitively, if a leading session s contains significantly
higher s compared with other leading sessions of Apps
in the leaderboard, it has high probability of having
ranking fraud. To capture this, we propose to apply
statistical hypothesis test for computing the significance
of s for each leading session. Specifically, we define
two statistical hypotheses as follows and compute the
p-value of each leading session.
B HYPOTHESIS 0: The signature s of leading session s is
not useful for detecting ranking fraud.
B HYPOTHESIS 1: The signature s of leading session s is
significantly greater than expectation.
Here, we propose to use the popular Gaussian ap-
proximation to compute the p-value with the above
hypotheses. Specifically, we assume s follows the Gaus-
sian distribution, s  N (; ); where  and  can
be learnt by the classic maximum-likelihood estimation
(MLE) method from the observations of s in all Apps’
historical leading sessions. Then, we can calculate the
p-value by
P
 
N (; )  s

= 1  1
2
 
1 + erf

s   

p
2
!
; (3)
where erf(x) is the Gaussian Error Function as follows,
erf(x) =
2p

Z x
0
e t
2
dt: (4)
Intuitively, a leading session with a smaller p-value P
has more chance to reject HYPOTHESIS 0 and accept
HYPOTHESIS 1. This means it has more chance of com-
mitting ranking fraud. Thus, we define the evidence as
	1(s) = 1  P
 N (; )  s: (5)
EVIDENCE 2. As discussed above, the Apps with
ranking fraud often have a short maintaining phase with
high ranking positions in each leading event. Thus, if
we denote the maintaining phase of a leading event e
as tem = (tec   teb + 1), and the average rank in this
maintaining phase as rem, we can define a fraud signature
s for each leading session as follows,
s =
1
jEsj
X
e2s
K   rem
tem
; (6)
where K is the ranking threshold in Definition 1.
If a leading session contains significantly higher s
compared with other leading sessions of Apps in the
leaderboard, it has high chance of having ranking fraud.
To capture such signatures, we define two statistical
hypotheses as follows to compute the significance of s
for each leading session.
B HYPOTHESIS 0: The signature s of leading session s is
not useful for detecting ranking fraud.
B HYPOTHESIS 1: The signature s of leading session s is
significantly higher than expectation.
Here, we also propose to use the Gaussian approxima-
tion to calculate the p-value with the above hypotheses.
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 5
Specifically, we assume s follows the Gaussian distri-
bution, s  N (; ); where  and  can be learnt
by the MLE method from the observations of s in all
Apps’ historical leading sessions. Then, we can calculate
the evidence by
	2(s) = 1  P
 N (; )  s: (7)
EVIDENCE 3. The number of leading events in a
leading session, i.e., jEsj, is also a strong signature of
ranking fraud. For a normal App, the recession phase
indicates the fading of popularity. Therefore, after the
end of a leading event, it is unlikely to appear another
leading event in a short time unless the App updates its
version or carries out some sales promotion. Therefore,
if a leading session contains much more leading events
compared with other leading sessions of Apps in the
leaderboard, it has high probability of having ranking
fraud. To capture this, we define two statistical hypothe-
ses to compute the significance of jEsj for each leading
session as follows.
B HYPOTHESIS 0: The signature jEsj of leading session s
is not useful for detecting ranking fraud.
B HYPOTHESIS 1: The signature jEsj of leading session s
is significantly lager than expectation.
Since jEsj always has discrete values, we propose to
leverage the Poisson approximation to calculate the p-
value with the above hypotheses. Specifically, we assume
jEsj follows the Poisson distribution, jEsj  P(s);
where the parameter s can be learnt by the MLE
method from the observations of jEsj in all Apps’ histor-
ical leading sessions. Then, we can calculate the p-value
as follows,
P
 
P(s)  jEsj

= 1  e s
jEsjX
i=0
(s)
i
i!
: (8)
Therefore, we can compute the evidence by
	3(s) = 1  P
 P(s)  jEsj: (9)
Intuitively, the values of the above three evidences
	1(s), 	2(s) and 	3(s) are all within the range of [0; 1].
Meanwhile, the higher evidence value a leading session
has, the higher probability this session contains ranking
fraud activities.
3.2 Rating based Evidences
The ranking based evidences are useful for ranking fraud
detection. However, sometimes, it is not sufficient to only
use ranking based evidences. For example, some Apps
created by the famous developers, such as Gameloft,
may have some leading events with large values of
1 due to the developers’ credibility and the “word-of-
mouth” advertising effect. Moreover, some of the legal
marketing services, such as “limited-time discount”, may
also result in significant ranking based evidences. To
solve this issue, we also study how to extract fraud
evidences from Apps’ historical rating records.
(a) Example 1 (b) Example 2
Fig. 5. Two real-world examples of the distribution of
Apps’ daily average ratings.
Specifically, after an App has been published, it can
be rated by any user who downloaded it. Indeed, user
rating is one of the most important features of App ad-
vertisement. An App which has higher rating may attract
more users to download and can also be ranked higher
in the leaderboard. Thus, rating manipulation is also an
important perspective of ranking fraud. Intuitively, if an
App has ranking fraud in a leading session s, the ratings
during the time period of s may have anomaly patterns
compared with its historical ratings, which can be used
for constructing rating based evidences. For example,
Figures 5 (a) and (b) show the distributions of the daily
average rating of a popular App “WhatsApp” and a
suspicious App discovered by our approach, respec-
tively. We can observe that a normal App always receives
similar average rating each day, while a fraudulent App
may receive relatively higher average ratings in some
time periods (e.g., leading sessions) than other times.
Thus, we define two rating fraud evidences based on
user rating behaviors as follows.
EVIDENCE 4. For a normal App, the average rating in
a specific leading session should be consistent with the
average value of all historical ratings. In contrast, an App
with rating manipulation might have surprisingly high
ratings in the fraudulent leading sessions with respect to
its historical ratings. Here, we define a fraud signature
Rs for each leading session as follows,
Rs = Rs  RaRa
; (s 2 a) (10)
where Rs is the average rating in leading session s, and
Ra is the average historical rating of App a. Therefore, if
a leading session has significantly higher value of Rs
compared with other leading sessions of Apps in the
leaderboard, it has high probability of having ranking
fraud. To capture this, we define statistical hypotheses to
compute the significance ofRs for each leading session
as follows.
B HYPOTHESIS 0: The signature Rs of leading session
s is not useful for detecting ranking fraud.
B HYPOTHESIS 1: The signature Rs of leading session
s is significantly higher than expectation.
Here, we use the Gaussian approximation to calculate
the p-value with the above hypotheses. Specifically, we
assume Rs follows the Gaussian distribution, Rs 
N (R; R); where R and R can be learnt by the
MLE method from the observations of Rs in all Apps’
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 6
historical leading sessions. Then, we can compute the
evidence by
	4(s) = 1  P
 N (R; R)  Rs: (11)
EVIDENCE 5. In the App rating records, each rating
can be categorized into jLj discrete rating levels, e.g., 1
to 5, which represent the user preferences of an App. The
rating distribution with respect to the rating level li in a
normal App a’s leading session s, p(lijRs;a), should be
consistent with the distribution in a’s historical rating
records, p(lijRa), and vice versa. Specifically, we can
compute the distribution by p(lijRs;a) =

Nsli
Ns
(:)

; where
Nsli is the number of ratings in s and the rating is at level
li, Ns(:) is the total number of ratings in s. Meanwhile,
we can compute p(lijRa) in a similar way. Then, we use
the Cosine similarity between p(lijRs;a) and p(lijRa) to
estimate the difference as follows.
D(s) =
PjLj
i=1 p(lijRs;a) p(lijRa)qPjLj
i=1 p(lijRs;a)2 
qPjLj
i=1 p(lijRa)2
: (12)
Therefore, if a leading session has significantly lower
value of D(s) compared with other leading sessions of
Apps in the leaderboard, it has high probability of hav-
ing ranking fraud. To capture this, we define statistical
hypotheses to compute the significance of D(s) for each
leading session as follows.
B HYPOTHESIS 0: The signature D(s) of leading session s
is not useful for detecting ranking fraud.
B HYPOTHESIS 1: The signature D(s) of leading session s
is significantly lower than expectation.
Here, we use the Gaussian approximation to compute
the p-value with the above hypotheses. Specifically, we
assume D(s) follows the Gaussian distribution, D(s) 
N (D; D), where D and D can be learnt by the
MLE method from the observations of D(s) in all Apps’
historical leading sessions. Then, we can compute the
evidence by
	5(s) = 1  P
 N (D; D)  D(s): (13)
The values of two evidences 	4(s) and 	5(s) are in
the range of [0; 1]. Meanwhile, the higher evidence value
a leading session has, the more chance this session has
ranking fraud activities.
3.3 Review based Evidences
Besides ratings, most of the App stores also allow users
to write some textual comments as App reviews. Such
reviews can reflect the personal perceptions and usage
experiences of existing users for particular mobile Apps.
Indeed, review manipulation is one of the most im-
portant perspective of App ranking fraud. Specifically,
before downloading or purchasing a new mobile App,
users often firstly read its historical reviews to ease
their decision making, and a mobile App contains more
positive reviews may attract more users to download.
Therefore, imposters often post fake reviews in the lead-
ing sessions of a specific App in order to inflate the App
downloads, and thus propel the App’s ranking position
in the leaderboard. Although some previous works on
review spam detection have been reported in recent
years [14], [19], [21], the problem of detecting the local
anomaly of reviews in the leading sessions and capturing
them as evidences for ranking fraud detection are still
under-explored. To this end, here we propose two fraud
evidences based on Apps’ review behaviors in leading
sessions for detecting ranking fraud.
EVIDENCE 6. Indeed, most of the the review manip-
ulations are implemented by bot farms due to the high
cost of human resource. Therefore, review spamers often
post multiple duplicate or near-duplicate reviews on the
same App to inflate downloads [19], [21]. In contrast,
the normal App always have diversified reviews since
users have different personal perceptions and usage
experiences. Based on the above observations, here we
define a fraud signature Sim(s), which denotes the
average mutual similarity between the reviews within
leading session s. Specifically, this fraud signature can
be computed by following steps.
First, for each review c in leading session s, we remove
all stop words (e.g., “of”, “the”) and normalize verbs and
adjectives (e.g., “plays! play”, “better ! good”).
Second, we build a normalized words vector  !wc =
dim[n] for each review c, where n indicates the number
of all unique normalized words in all reviews of s. To
be specific, here we have dim[i] = freqi;cP
i freqi;c
(1  i  n),
where freqi;c is the frequency of the i-th word in c.
Finally, we can calculated the similarity between two
reviews ci and cj by the Cosine similarity Cos( !wci ; !wcj ).
Thus, the fraud signature Sim(s) can be computed by
Sim(s) =
2P1i<jNs Cos( !wci ; !wcj )
Ns  (Ns   1) ; (14)
where Ns is the number of reviews during leading
session s. Intuitively, the higher value of sim(s) indicates
more duplicate/near-duplicate reviews in s. Thus, if a
leading session has significantly higher value of Sim(s)
compared with other leading sessions of Apps in the
leaderboard, it has high probability of having ranking
fraud. To capture this, we define statistical hypotheses
to compute the significance of Sim(s) for each leading
session as follows.
B HYPOTHESIS 0: The signature Sim(s) of leading session
s is not useful for detecting ranking fraud.
B HYPOTHESIS 1: The signature Sim(s) of leading session
s is significantly higher than expectation.
Here, we use the Gaussian approximation to com-
pute the p-value with the above hypotheses. Specifically,
we assume Sim(s) follows the Gaussian distribution,
Sim(s)  N (Sim; Sim), where Sim and Sim can be
learnt by the MLE method from the observations of
Sim(s) in all Apps’ historical leading sessions. Then, we
can compute the evidence by
	6(s) = 1  P
 N (Sim; Sim)  Sim(s): (15)
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 7
EVIDENCE 7. From the real-world observations, we
find that each review c is always associated with a
specific latent topic z. For example, some reviews may
be related to the latent topic “worth to play” while
some may be related to the latent topic “very boring”.
Meanwhile, since different users have different personal
preferences of mobile Apps, each App a may have
different topic distributions in their historical review
records. Intuitively, the topic distribution of reviews in
a normal leading session s of App a, i.e., p(zjs), should
be consistent with the topic distribution in all historical
review records of a, i.e., p(zja). It is because that the
review topics are based on the users’ personal usage
experiences but not the popularity of mobile Apps. In
contrast, if the reviews of s have been manipulated, the
two topic distributions will be markedly different. For
example, there may contain more positive topics, such as
“worth to play” and “popular”, in the leading session.
In this paper we propose to leverage topic modeling
to extract the latent topics of reviews. Specifically, here
we adopt the widely used Latent Dirichlet Allocation
(LDA) model [9] for learning latent semantic topics. To
be more specific, the historical reviews of a mobile App
a, i.e., Ca, is assumed to be generated as follows. First,
before generating Ca, K prior conditional distributions
of words given latent topics fzg are generated from a
prior Dirichlet distribution . Second, a prior latent topic
distribution a is generated from a prior Dirichlet distri-
bution  for each mobile App a. Then, for generating
the j-th word in Ca denoted as wa;j , the model firstly
generates a latent topic z from a and then generates
wa;j from z . The training process of LDA model is
to learn proper latent variables  = fP (zjCa)g and
 = fP (wjz)g for maximizing the posterior distribution
of review observations, i.e., P (Caj; ; ; ). In this paper,
we use a Markov chain Monte Carlo method named
Gibbs sampling [12] for training LDA model. If we
denote the reviews in leading session s of a as Cs;a, we
can use the KL-divergence to estimate the difference of
topic distributions between Ca and Cs;a.
DKL(sjja) =
X
k
P (zkjCs;a)lnP (zkjCs;a)
P (zkjCa) ; (16)
where P (zkjCa) and P (zkjCs;a) / P (zk)
Q
w2Cs;a P (wjzk)
can be obtained through the LDA training process. The
higher value of DKL(sjja) indicates the higher difference
of topic distributions between Ca and Cs;a. Therefore,
if a leading session has significantly higher value of
DKL(sjja) compared with other leading sessions of Apps
in the leaderboard, it has high probability of having
ranking fraud. To capture this, we define statistical hy-
potheses to compute the significance of DKL(sjja) for
each leading session as follows.
B HYPOTHESIS 0: The signature DKL(sjja) of leading
session s is not useful for detecting ranking fraud.
B HYPOTHESIS 1: The signature DKL(sjja) of leading
session s is significantly higher than expectation.
Here, we also use the Gaussian approximation to com-
pute the p-value with the above hypotheses. Specifically,
we assume DKL(sjja) follows the Gaussian distribution,
DKL(sjja)  N (DL; DL), where DL and DL can
be learnt by the MLE method from the observations of
DKL(sjja) in all Apps’ historical leading sessions. Then,
we can compute the evidence by
	7(s) = 1  P
 N (DL; DL)  DKL(sjja): (17)
The values of two evidences 	6(s) and 	7(s) are in
the range of [0; 1]. Meanwhile, the higher evidence value
a leading session has, the more chance this session has
ranking fraud activities.
3.4 Evidence Aggregation
After extracting three types of fraud evidences, the next
challenge is how to combine them for ranking fraud
detection. Indeed, there are many ranking and evidence
aggregation methods in the literature, such as permu-
tation based models [17], [18], score based models [11],
[26] and Dempster-Shafer rules [10], [23]. However, some
of these methods focus on learning a global ranking for
all candidates. This is not proper for detecting ranking
fraud for new Apps. Other methods are based on super-
vised learning techniques, which depend on the labeled
training data and are hard to be exploited. Instead,
we propose an unsupervised approach based on fraud
similarity to combine these evidences.
Specifically, we define the final evidence score 	(s)
as a linear combination of all the existing evidences as
Equation 18. Note that, here we propose to use the linear
combination because it has been proven to be effective
and is widely used in relevant domains, such as ranking
aggregation [16], [20].
	(s) =
N	X
i=1
wi 	i(s); s:t:
N	X
i=1
wi = 1; (18)
where N	 = 7 is the number of evidences, and weight
wi 2 [0; 1] is the aggregation parameter of evidence
	i(s). Thus, the problem of evidence aggregation be-
comes how to learn the proper parameters fwig from
the training leading sessions.
We first propose an intuitive assumption as Principle
1 for our evidence aggregation approach. Specifically,
we assume that effective evidences should have similar
evidence scores for each leading session, while poor evidences
will generate different scores from others. In other words,
evidences that tend to be consistent with the plurality
of evidences will be given higher weights and evidences
which tend to disagree will be given smaller weights. To
this end, for each evidence score 	i(s), we can measure
its consistence using the variance-like measure
i(s) =
 
	i(s) 	(s)
2
; (19)
where 	(s) is the average evidence score of leading
session s obtained from all N	 evidences. If i(s) is
small, the corresponding 	i(s) should be given a bigger
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 8
weight and vice versa. Therefore, given an App set
A = faig with their leading sessions fsjg, we can define
the evidence aggregation problem as an optimization
problem that minimizes weighted variances of the ev-
idences over all leading sessions; that is
argmin
w
X
a2A
X
s2a
N	X
i=1
wi  i(s); (20)
s:t:
N	X
i=1
wi = 1; 8wi  0: (21)
In this paper, we exploit the gradient based approach
with exponentiated updating [15], [16] to solve this
problem. To be specific, we first assign wi = 1N	 as
the initial value, then for each s, we can compute the
gradient by,
ri = @wi  i(s)
@wi
= i(s): (22)
Thus, we can update the weight wi by
wi =
wi  exp( ri)PN	
j=1 w

j  exp( rj)
; (23)
where wi is the last updated weight value wi, and  is
the learning rate, which is empirically set  = 10 2 in
our experiments.
Finally, we can exploit Equation (18) to estimate the
final evidence score of each leading session. Moreover,
given a leading session s with a predefined threshold  ,
we can determine that s has ranking fraud if 	(s) >  .
However, sometimes only using evidence scores for
evidence aggregation is not appropriate. It is because
that different evidences may have different score range
to evaluate leading sessions. For example, some evi-
dences may always generate higher scores for leading
sessions than the average evidence score, although they
can detect fraudulent leading sessions and rank them in
accurate positions.
Therefore, here we propose another assumption as
Principle 2 for our evidence aggregation approach.
Specifically, we assume that effective evidences should rank
leading sessions from a similar conditional distribution, while
poor evidences will lead to a more uniformly random ranking
distribution [16]. To this end, given a set of leading
sessions, we first rank them by each evidence score and
obtain N	 ranked lists. Let us denote i(s) as the ranking
of session s returned by 	i(s), then we can calculate the
average ranking for leading session s by
(s) =
1
N	
N	X
i=1
i(s): (24)
Then, for each evidence score 	i(s), we can measure its
consistence using the variance-like measure,
i (s) =
 
i(s)  (s)
2
: (25)
If i (s) is small, the corresponding 	i(s) should be given
a bigger weight and vice versa. Then we can replace i(s)
by i (s) in Equation 20, and exploit similar gradient
based approach that is introduced above for learning the
weights of evidences.
4 DISCUSSION
Here, we provide some discussion about the proposed
ranking fraud detection system for mobile Apps.
First, the download information is an important sig-
nature for detecting ranking fraud, since ranking ma-
nipulation is to use so-called “bot farms” or “human
water armies” to inflate the App downloads and ratings
in a very short time. However, the instant download
information of each mobile App is often not available for
analysis. In fact, Apple and Google do not provide ac-
curate download information on any App. Furthermore,
the App developers themselves are also reluctant to
release their download information for various reasons.
Therefore, in this paper, we mainly focus on extracting
evidences from Apps’ historical ranking, rating and re-
view records for ranking fraud detection. However, our
approach is scalable for integrating other evidences if
available, such as the evidences based on the download
information and App developers’ reputation.
Second, the proposed approach can detect ranking
fraud happened in Apps’ historical leading sessions.
However, sometime, we need to detect such ranking
fraud from Apps’ current ranking observations. Actually,
given the current ranking ranow of an App a, we can
detect ranking fraud for it in two different cases. First,
if ranow > K, where K is the ranking threshold intro-
duced in Definition 1, we believe a does not involve in
ranking fraud, since it is not in a leading event. Second,
if ranow < K, which means a is in a new leading event e,
we treat this case as a special case that teend = t
e
now and
2 = 0. Therefore, such real-time ranking frauds also can
be detected by the proposed approach.
Finally, after detecting ranking fraud for each leading
session of a mobile App, the remainder problem is
how to estimate the credibility of this App. Indeed, our
approach can discover the local anomaly instead of the
global anomaly of mobile Apps. Thus, we should take
consideration of such kind of local characteristics when
estimating the credibility of Apps. To be specific, we
define an App fraud score F(a) for each App a according
to howmany leading sessions of a contain ranking fraud.
F(a) =
X
s2a
[[	(s) >  ]]	(s)ts; (26)
where s 2 a denotes that s is a leading session of
App a, and 	(s) is the final evidence score of leading
session s that can be calculated by Equation 18. In
particular, we define a signal function [[x]] (i.e., [[x]] = 1
if x = True, and 0 otherwise) and a fraud threshold  to
decide the top k fraudulent leading sessions. Moreover,
ts = (tsend   tsstart + 1) is the time range of s, which
indicates the duration of ranking fraud. Intuitively, an
App contains more leading sessions, which have high
fraud evidence scores and long time duration, will have
higher App fraud scores.
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 9
(a) Top Free 300 data set (b) Top Paid 300 data set
Fig. 6. The distribution of the number of Apps w.r.t
different rankings.
(a) Top Free 300 data set (b) Top Paid 300 data set
Fig. 7. The distribution of the number of Apps w.r.t
different numbers of ratings.
5 EXPERIMENTAL RESULTS
In this section, we evaluate the performances of ranking
fraud detection using real-world App data.
5.1 The Experimental Data
The experimental data sets were collected from the “Top
Free 300” and “Top Paid 300” leaderboards of Apple’s
App Store (U.S.) from February 2, 2010 to September 17,
2012. The data sets contain the daily chart rankings 1 of
top 300 free Apps and top 300 paid Apps, respectively.
Moreover, each data set also contains the user ratings
and review information. Table 1 shows the detailed data
characteristics of our data sets.
TABLE 1
Statistics of the experimental data.
Top Free 300 Top Paid 300
App Num. 9,784 5,261
Ranking Num. 285,900 285,900
Avg. Ranking Num. 29.22 54.34
Rating Num. 14,912,459 4,561,943
Avg. Rating Num. 1,524.17 867.12
Figures 6 (a) and 6 (b) show the distributions of the
number of Apps with respect to different rankings in
these data sets. In the figures, we can see that the
number of Apps with low rankings is more than that
of Apps with high rankings. Moreover, the competition
between free Apps is more than that between paid Apps,
especially in high rankings (e.g., top 25). Figures 7 (a)
and 7 (b) show the distribution of the number of Apps
with respect to different number of ratings in these data
sets. In the figures, we can see that the distribution of
App ratings is not even, which indicates that only a small
percentage of Apps are very popular.
1. The information was collected at 11:00PM (PST) each day.
(a) Top Free 300 data set (b) Top Paid 300 data set
Fig. 8. The distribution of the number of Apps w.r.t
different numbers of leading events.
(a) Top Free 300 data set (b) Top Paid 300 data set
Fig. 9. The distribution of the number of Apps w.r.t
different number of leading sessions.
5.2 Mining Leading Sessions
Here, we demonstrate the results of mining leading
sessions in both data sets. Specifically, in Algorithm 1,
we set the ranking threshold K = 300 and threshold
 = 7. This denotes two adjacent leading events can be
segmented into the same leading session if they occur
within one week of each other. Figure 8 and Figure 9
show the distributions of the number of Apps with
respect to different numbers of contained leading events
and leading sessions in both data sets. In these figures,
we can see that only a few Apps have many leading
events and leading sessions. The average numbers of
leading events and leading sessions are 2:69 and 1:57 for
free Apps, and 4:20 and 1:86 for paid Apps. Moreover,
Figures 10 (a) and 10 (b) show the distribution of the
number of leading sessions with respect to different
numbers of contained leading events in both data sets.
In these figures, we can find only a few leading sessions
contain many leading events. This also validates the
evidence 	3. Indeed, the average number of leading
events in each leading session is 1:70 for free Apps and
2:26 for paid Apps.
5.3 Human Judgement based Evaluation
To the best of our knowledge, there is no existing
benchmark to decide which leading sessions or Apps
really contain ranking fraud. Thus, we develop four
intuitive baselines and invite five human evaluators
to validate the effectiveness of our approach EA-RFD
(Evidence Aggregation based Ranking Fraud Detection).
Particularly, we denote our approach with score based
aggregation (i.e., Principle 1) as EA-RFD-1, and our
approach with rank based aggregation (i.e., Principle 2)
as EA-RFD-2, respectively.
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 10
(a) Top Free 300 data set (b) Top Paid 300 data set
Fig. 10. The distribution of the number of leading ses-
sions w.r.t different number of leading events.
5.3.1 Baselines
The first baseline Ranking-RFD stands for Ranking ev-
idence based Ranking Fraud Detection, which estimates
ranking fraud for each leading session by only using
ranking based evidences (i.e., 	1 to 	3). These three
evidences are integrated by our aggregation approach.
The second baseline Rating-RFD stands for Rating
evidence based Ranking Fraud Detection, which esti-
mates the ranking fraud for each leading session by only
using rating based evidences (i.e., 	4 and 	5). These two
evidences are integrated by our aggregation approach.
The third baseline Review-RFD stands for Review ev-
idence based Ranking Fraud Detection, which estimates
the ranking fraud for each leading session by only using
review based evidences (i.e., 	6 and 	7). These two
evidences are integrated by our aggregation approach.
Particularly, here we only use the rank based aggrega-
tion approach (i.e., Principle 2) for integrating evidences
in above baselines. It is because that these baselines are
mainly used for evaluating the effectiveness of different
kinds of evidences, and our preliminary experiments val-
idated that baselines with Principle 2 always outperform
baselines with Principle 1.
The last baseline E-RFD stands for Evidence based
Ranking Fraud Detection, which estimates the ranking
fraud for each leading session by ranking, rating and
review based evidences without evidence aggregation.
Specifically, it ranks leading sessions by Equation 18,
where each wi is set to be 1=7 equally. This baseline
is used for evaluating the effectiveness of our ranking
aggregation method.
Note that, according to Definition 3, we need to define
some ranking ranges before extracting ranking based
evidences for EA-RFD-1, EA-RFD-2, Rank-RFD and E-
RFD. In our experiments, we segment the rankings into
5 different ranges, i.e., [1; 10], [11; 25], [26; 50], [51; 100],
[101; 300], which are commonly used in App leader-
boards. Furthermore, we use the LDA model to extract
review topics as introduced in Section 3.3. Particularly,
we first normalize each review by the Stop-Words Re-
mover [6] and the Porter Stemmer [7]. Then, the number
of latent topic Kz is set to 20 according to the perplexity
based estimation approach [8], [31]. Two parameters 
and  for training LDA model are set to be 50=K and
0.1 according to [13].
Fig. 11. The screenshots of our fraud evaluation platform.
5.3.2 The Experimental Setup
To study the performance of ranking fraud detection by
each approach, we set up the evaluation as follows.
First, for each approach, we selected 50 top ranked
leading sessions (i.e., most suspicious sessions), 50 mid-
dle ranked leading sessions (i.e., most uncertain ses-
sions), and 50 bottom ranked leading sessions (i.e., most
normal sessions) from each data set. Then, we merged
all the selected sessions into a pool which consists 587
unique sessions from 281 unique Apps in “Top Free 300”
data set, and 541 unique sessions from 213 unique Apps
in “Top Paid 300” data set.
Second, we invited five human evaluators who are
familiar with Apple’s App store and mobile Apps to
manually label the selected leading sessions with score
2 (i.e., Fraud), 1 (i.e., Not Sure) and 0 (i.e., Non-
fraud). Specifically, for each selected leading session,
each evaluator gave a proper score by comprehensively
considering the profile information of the App (e.g.,
descriptions, screenshots), the trend of rankings during
this session, the App leaderboard information during
this session, the trend of ratings during this session, and
the reviews during this session. Moreover, they can also
download and try the corresponding Apps for obtaining
user experiences. Particularly, to facilitate their evalu-
ation, we develop a Ranking Fraud Evaluation Platform,
which ensures that the evaluators can easily browse
all the information. Also, the platform demonstrates
leading sessions in random orders, which guarantees
there is no relationship between leading sessions’ order
and their fraud scores. Figure 11 shows the screenshot
of the platform. The left panel shows the main menu,
the right upper panel shows the reviews for the given
session, and the right lower panel shows the ranking
related information for the given session. After human
evaluation, each leading session s is assigned a fraud
score f(s) 2 [0; 10]. As a result, all the five evaluators
agreed on 86 fraud sessions and 113 non-fraud sessions
Top Free 300 data set. Note that, 11 labeled fraud sessions
among them are from the external reported suspicious
Apps [4], [5], which validates the effectiveness of our hu-
man judgement. Similarly, all the five evaluators agreed
on 94 fraud sessions and 119 non-fraud sessions Top Free
300 data set. Moreover, we computed the Cohen’s kappa
coefficient [1] between each pair of evaluators to estimate
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 11
(a) Precision@K (b) Recall@K (c) F@K (d) NDCG@K
Fig. 12. The overall performance of each detection approach in Top Free 300 data set.
(a) Precision@K (b) Recall@K (c) F@K (d) NDCG@K
Fig. 13. The overall performance of each detection approach in Top Paid 300 data set.
the inter-evaluator agreement. The values of Cohen’s
kappa coefficient are between 0:66 to 0:72 in the user
evaluation. This indicates the substantial agreement [19].
Finally, we further ranked the leading sessions by each
approach with respect to their fraudulent scores, and
obtained six ranked lists of leading sessions. In partic-
ular, if we treat the commonly agreed fraud sessions
(i.e., 89 sessions in Top Free 300 data set, 94 sessions
in Top Paid 300 data set) as the ground truth, we can
evaluate each approach with three widely-used metrics,
namely Precision@K, Recall@K, F@K [2]. Also, we can
exploit the metric Normalized Discounted Cumulative
Gain (NDCG) for determining the ranking performance
of each approach. Specifically, the discounted cumula-
tive gain given a cut-off rank K can be calculated by
DCG@K =
PK
i=1
2f(si) 1
log2(1+i)
; where f(si) is the human
labeled fraud score. The NDCG@K is the DCG@K
normalized by the IDCG@K, which is the DCG@K
value of the ideal ranking list of the returned results, i.e.,
we have NDCG@K = DCG@KIDCG@K : NDCG@K indicates
how well the rank order of given sessions returned by an
approach with a cut-off rank K. The larger NDCG@K
value, the better performance of ranking fraud detection.
5.3.3 Overall Performances
In this subsection, we present the overall performances
of each ranking fraud detection approach with re-
spect to different evaluation metrics, i.e., Precision@K,
Recall@K, F@k, and NDCG@K. Particularly, here we
set the maximum K to be 200, and all experiments are
conducted on a 2.8GHZ2 quad-core CPU, 4G main
memory PC.
Figures 12 and Figures 13 show the evaluation per-
formance of each detection approach in two data sets.
From these figures we can observe that the evaluation
results in two data sets are consistent. Indeed, by an-
alyzing the evaluation results, we can obtain several
insightful observations. Specifically, first, we find that
our approach, i.e., EA-RFD-2/EA-RFD-2, consistently
outperforms other baselines and the improvements are
more significant for smallerK (e.g.,K < 100). This result
clearly validates the effectiveness of our evidence ag-
gregation based framework for detecting ranking fraud.
Second, EA-RFD-2 outperforms EA-RFD-1 sightly in
terms of all evaluation metrics, which indicates that rank
based aggregation (i.e., Principle 2) is more effective than
score based aggregation (i.e., Principle 1) for integrating
fraud evidences. Third, our approach consistently out-
performs E-RFD, which validates the effectiveness of ev-
idence aggradation for detecting ranking fraud. Fourth,
E-RFD have better detection performance than Ranking-
RFD, Rating-RFD and Review-RFD. This indicates that
leveraging three kinds of evidences is more effective than
only using one type of evidences, even if without evi-
dence aggregation. Finally, by comparing Ranking-RFD,
Rating-RFD and Review-RFD, we can observe that the
ranking based evidences are more effective than rating
and review based evidences. It is because rating and re-
view manipulations are only supplementary to ranking
manipulation. Particularly, we observe that Review-RFD
may not be able to lead to the good performance in
terms of all evaluation metrics on the two data sets. A
possible reason behind this phenomenon is that review
manipulation (i.e., fake-positive reviews) does not di-
rectly affect the chart ranking of Apps, but may increase
the possibility of inflating App downloads and ratings.
Therefore, the review manipulation does not necessarily
result in ranking fraud due to the unknown ranking
principles in the App Store. However, the proposed
review based evidences can be helpful as supplementary
for ranking fraud detection. Actually, in our preliminary
experiments, we found that the review based evidences
could always improve the detection performances while
being used together with other evidences. This clearly
validates the effectiveness of the review based evidences.
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 12
Fig. 14. Case study of reported suspicious mobile Apps.
To further validate the experimental results, we also
conduct a series of paired T-test of 0.95 confidence level
which show that the improvements of our approach, i.e.,
EA-RFD-2/EA-RFD-1, on all evaluation metrics with dif-
ferent K compared to other baselines are all statistically
significant.
5.4 Case Study: Evaluating App Credibility
As introduced in Section 4, our approach can be used
for evaluating the credibility of Apps by Equation 26.
Here, we study the performance of evaluating App
credibility based on the prior knowledge from existing
reports. Specifically, as reported by IBTimes [5], there are
eight free Apps which might involve in ranking fraud.
In this paper, we use seven of them in our data set
(Tiny Pets, Social Girl, Fluff Friends, Crime City, VIP Poker,
Sweet Shop, Top Girl) for evaluation. Indeed, we try to
study whether each approach can find these suspicious
Apps with high rankings, since a good ranking fraud
detection system should have the capability of capturing
these suspicious Apps. Particularly, instead of setting
a fixed fraud threshold  in Equation 26, we treat top
10% ranked leading sessions as suspicious sessions to
compute the credibility of each App.
Figure 14 shows the top percentage position of each
App in the ranked list returned by each approach. We
can see that our approach, i.e., EA-RFD-2 and EA-RFD-
1, can rank those suspicious Apps into higher positions
than other baseline methods. Similarly as the results in
Section 5.3.3, only leveraging single kind of evidences
for fraud detection cannot obtain good performance, i.e.,
finding such suspicious Apps in high positions.
Figure 15 shows the ranking records of the above Apps
(limited by space, we only show four of them). In this
figure, we find all these Apps have clear ranking based
fraud evidences. For example, some Apps have very
short leading sessions with high rankings (i.e., Evidence
1 and 2), and some Apps have leading session with many
leading events (i.e., Evidence 3). These observations
clearly validate the effectiveness of our approach.
5.5 Efficiency and Robustness of our Approach
The computational cost of our approach majorally comes
from the task of extracting three kinds of fraud evidences
for the given leading sessions. Indeed, the main pro-
cesses of this task can be calculated offline in advance.
For example, the LDA model can be trained offline and
the fraud signatures of the existing leading sessions can
also be mined in advance and stored in the server. In this
(a) Fluff Friends (b) VIP Poker
(c) Tiny Pets (d) Crime City
Fig. 15. The demonstration of the ranking records of four
reported suspicious Apps.
(a) Top Free 300 data set (b) Top Paid 300 data set
Fig. 16. The robustness test of our aggregation model
with two principles.
case, the process of extracting evidences for each leading
session will be very fast (less than 100 millisecond on
average in our experiments).
Meanwhile, a learning process is required for evidence
aggregation. After learning the aggregation model on
a historical data set, each new test App can reuse this
model for detecting ranking fraud. However, it is still not
clear how many learning data are required. To study this
problem and validate the robustness of our approach, we
first rank all leading sessions by modeling with weight
parameters learnt from the entire data set. Then we
also rank all leading sessions by modeling with weight
parameters learnt from different segmentation of the
entire data set (i.e., 10%,...,100%). Finally, we test the root
mean squared error (RMSE) of the ranking of leading
sessions between different results. Figure 16 shows the
results of robust test on two data sets. We can find that
the aggregation model does not need a lot of learning
data, thus the robustness of our approach is reasonable.
6 RELATED WORK
Generally speaking, the related works of this study can
be grouped into three categories.
The first category is about Web ranking spam detec-
tion. Specifically, the Web ranking spam refers to any
deliberate actions which bring to selected Web pages
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 13
an unjustifiable favorable relevance or importance [30].
For example, Ntoulas et al. [22] have studied various
aspects of content-based spam on the Web and presented
a number of heuristic methods for detecting content
based spam. Zhou et al [30] have studied the problem
of unsupervised Web ranking spam detection. Specifi-
cally, they proposed an efficient online link spam and
term spam detection methods using spamicity. Recently,
Spirin et al. [25] have reported a survey on Web spam
detection, which comprehensively introduces the princi-
ples and algorithms in the literature. Indeed, the work
of Web ranking spam detection is mainly based on the
analysis of ranking principles of search engines, such as
PageRank and query term frequency. This is different
from ranking fraud detection for mobile Apps.
The second category is focused on detecting online
review spam. For example, Lim et al. [19] have identified
several representative behaviors of review spammers
and model these behaviors to detect the spammers. Wu
et al. [27] have studied the problem of detecting hybrid
shilling attacks on rating data. The proposed approach
is based on the semi-supervised learning and can be
used for trustworthy product recommendation. Xie et
al. [28] have studied the problem of singleton review
spam detection. Specifically, they solved this problem
by detecting the co-anomaly patterns in multiple review
based time series. Although some of above approaches
can be used for anomaly detection from historical rating
and review records, they are not able to extract fraud
evidences for a given time period (i.e., leading session).
Finally, the third category includes the studies on
mobile App recommendation. For example, Yan et al. [29]
developed a mobile App recommender system, named
Appjoy, which is based on user’s App usage records to
build a preference matrix instead of using explicit user
ratings. Also, to solve the sparsity problem of App usage
records, Shi et al. [24] studied several recommendation
models and proposed a content based collaborative fil-
tering model, named Eigenapp, for recommending Apps
in their Web site Getjar. In addition, some researchers
studied the problem of exploiting enriched contextual
information for mobile App recommendation. For exam-
ple, Zhu et al. [32] proposed a uniform framework for
personalized context-aware recommendation, which can
integrate both context independency and dependency
assumptions. However, to the best of our knowledge,
none of previous works has studied the problem of
ranking fraud detection for mobile Apps.
7 CONCLUDING REMARKS
In this paper, we developed a ranking fraud detection
system for mobile Apps. Specifically, we first showed
that ranking fraud happened in leading sessions and
provided a method for mining leading sessions for each
App from its historical ranking records. Then, we iden-
tified ranking based evidences, rating based evidences
and review based evidences for detecting ranking fraud.
Moreover, we proposed an optimization based aggrega-
tion method to integrate all the evidences for evaluating
the credibility of leading sessions from mobile Apps.
An unique perspective of this approach is that all the
evidences can be modeled by statistical hypothesis tests,
thus it is easy to be extended with other evidences
from domain knowledge to detect ranking fraud. Finally,
we validate the proposed system with extensive experi-
ments on real-world App data collected from the Apple’s
App store. Experimental results showed the effectiveness
of the proposed approach.
In the future, we plan to study more effective fraud
evidences and analyze the latent relationship among rat-
ing, review and rankings. Moreover, we will extend our
ranking fraud detection approach with other mobile App
related services, such as mobile Apps recommendation,
for enhancing user experience.
Acknowledgement. This work was supported in part by grants
from National Science Foundation for Distinguished Young Scholars
of China (Grant No. 61325010), Natural Science Foundation of China
(NSFC, Grant No.71329201), National High Technology Research and
Development Program of China (Grant No. SS2014AA012303), Sci-
ence and Technology Development of Anhui Province (Grants No.
1301022064), the International Science and Technology Cooperation
Plan of Anhui Province (Grant No. 1303063008). This work was also
partially supported by grants from National Science Foundation (NSF)
via grant numbers CCF-1018151 and IIS-1256016.
REFERENCES
[1] http://en.wikipedia.org/wiki/cohen’s kappa.
[2] http://en.wikipedia.org/wiki/information retrieval.
[3] https://developer.apple.com/news/index.php?id=02062012a.
[4] http://venturebeat.com/2012/07/03/apples-crackdown-on-app-
ranking-manipulation/.
[5] http://www.ibtimes.com/apple-threatens-crackdown-biggest-
app-store-ranking-fraud-406764.
[6] http://www.lextek.com/manuals/onix/index.html.
[7] http://www.ling.gu.se/l˜ager/mogul/porter-stemmer.
[8] L. Azzopardi, M. Girolami, and K. V. Risjbergen. Investigating the
relationship between language model perplexity and ir precision-
recall measures. In Proceedings of the 26th International Conference on
Research and Development in Information Retrieval (SIGIR’03), pages
369–370, 2003.
[9] D. M. Blei, A. Y. Ng, and M. I. Jordan. Lantent dirichlet allocation.
Journal of Machine Learning Research, pages 993–1022, 2003.
[10] Y. Ge, H. Xiong, C. Liu, and Z.-H. Zhou. A taxi driving fraud
detection system. In Proceedings of the 2011 IEEE 11th International
Conference on Data Mining, ICDM ’11, pages 181–190, 2011.
[11] D. F. Gleich and L.-h. Lim. Rank aggregation via nuclear norm
minimization. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining, KDD ’11, pages
60–68, 2011.
[12] T. L. Griffiths and M. Steyvers. Finding scientific topics. In Proc.
of National Academy of Science of the USA, pages 5228–5235, 2004.
[13] G. Heinrich. Paramter stimaion for text analysis. Technical report,
University of Lipzig, 2008.
[14] N. Jindal and B. Liu. Opinion spam and analysis. In Proceedings
of the 2008 International Conference on Web Search and Data Mining,
WSDM ’08, pages 219–230, 2008.
[15] J. Kivinen and M. K. Warmuth. Additive versus exponentiated
gradient updates for linear prediction. In Proceedings of the twenty-
seventh annual ACM symposium on Theory of computing, STOC ’95,
pages 209–218, 1995.
[16] A. Klementiev, D. Roth, and K. Small. An unsupervised learning
algorithm for rank aggregation. In Proceedings of the 18th European
conference on Machine Learning, ECML ’07, pages 616–623, 2007.
1041-4347 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/TKDE.2014.2320733, IEEE Transactions on Knowledge and Data Engineering
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, XX XXXX 14
[17] A. Klementiev, D. Roth, and K. Small. Unsupervised rank
aggregation with distance-based models. In Proceedings of the 25th
international conference on Machine learning, ICML ’08, pages 472–
479, 2008.
[18] A. Klementiev, D. Roth, K. Small, and I. Titov. Unsupervised rank
aggregation with domain-specific expertise. In Proceedings of the
21st international jont conference on Artifical intelligence, IJCAI’09,
pages 1101–1106, 2009.
[19] E.-P. Lim, V.-A. Nguyen, N. Jindal, B. Liu, and H. W. Lauw.
Detecting product review spammers using rating behaviors. In
Proceedings of the 19th ACM international conference on Information
and knowledge management, CIKM ’10, pages 939–948, 2010.
[20] Y.-T. Liu, T.-Y. Liu, T. Qin, Z.-M. Ma, and H. Li. Supervised rank
aggregation. In Proceedings of the 16th international conference on
World Wide Web, WWW ’07, pages 481–490, 2007.
[21] A. Mukherjee, A. Kumar, B. Liu, J. Wang, M. Hsu, M. Castellanos,
and R. Ghosh. Spotting opinion spammers using behavioral
footprints. In Proceedings of the 19th ACM SIGKDD international
conference on Knowledge discovery and data mining, KDD ’13, 2013.
[22] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. Detecting
spam web pages through content analysis. In Proceedings of the
15th international conference on World Wide Web, WWW ’06, pages
83–92, 2006.
[23] G. Shafer. A mathematical theory of evidence. 1976.
[24] K. Shi and K. Ali. Getjar mobile application recommendations
with very sparse datasets. In Proceedings of the 18th ACM SIGKDD
international conference on Knowledge discovery and data mining,
KDD ’12, pages 204–212, 2012.
[25] N. Spirin and J. Han. Survey on web spam detection: principles
and algorithms. SIGKDD Explor. Newsl., 13(2):50–64, May 2012.
[26] M. N. Volkovs and R. S. Zemel. A flexible generative model
for preference aggregation. In Proceedings of the 21st international
conference on World Wide Web, WWW ’12, pages 479–488, 2012.
[27] Z. Wu, J. Wu, J. Cao, and D. Tao. Hysad: a semi-supervised hybrid
shilling attack detector for trustworthy product recommendation.
In Proceedings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’12, pages 985–993, 2012.
[28] S. Xie, G. Wang, S. Lin, and P. S. Yu. Review spam detection
via temporal pattern discovery. In Proceedings of the 18th ACM
SIGKDD international conference on Knowledge discovery and data
mining, KDD ’12, pages 823–831, 2012.
[29] B. Yan and G. Chen. Appjoy: personalized mobile application
discovery. In Proceedings of the 9th international conference on Mobile
systems, applications, and services, MobiSys ’11, pages 113–126, 2011.
[30] B. Zhou, J. Pei, and Z. Tang. A spamicity approach to web spam
detection. In Proceedings of the 2008 SIAM International Conference
on Data Mining, SDM’08, pages 277–288, 2008.
[31] H. Zhu, H. Cao, E. Chen, H. Xiong, and J. Tian. Exploiting
enriched contextual information for mobile app classification. In
Proceedings of the 21st ACM international conference on Information
and knowledge management, CIKM ’12, pages 1617–1621, 2012.
[32] H. Zhu, E. Chen, K. Yu, H. Cao, H. Xiong, and J. Tian. Mining
personal context-aware preferences for mobile users. In Data
Mining (ICDM), 2012 IEEE 12th International Conference on, pages
1212–1217, 2012.
[33] H. Zhu, H. Xiong, Y. Ge, and E. Chen. Ranking fraud detection
for mobile apps: A holistic view. In Proceedings of the 22nd ACM
international conference on Information and knowledge management,
CIKM ’13, 2013.
Hengshu Zhu is currently a Ph.D. student in
the School of Computer Science and Technol-
ogy at University of Science and Technology of
China (USTC), China. He was supported by the
China Scholarship Council (CSC) as a visiting
research student at Rutgers, the State University
of New Jersey, USA, for more than one year. He
received his B.E. degree in Computer Science
from USTC, China, in 2009.
His main research interests include mobile
data mining, recommender systems, and social
networks. During his Ph.D. study, he received the KSEM-2011 and
WAIM-2013 Best Student Paper Award. He has published a number of
papers in refereed journals and conference proceedings, such as IEEE
TMC, ACM TIST, WWW Journal, KAIS, ACM CIKM, and IEEE ICDM.
He also has served as a reviewer for numerous journals, such as IEEE
TSMC-B, KAIS, and WWW Journal.
Hui Xiong is currently an Associate Professor
and Vice Chair of the Management Science
and Information Systems Department, and the
Director of Rutgers Center for Information As-
surance at the Rutgers, the State University
of New Jersey, where he received a two-year
early promotion/tenure (2009), the Rutgers Uni-
versity Board of Trustees Research Fellowship
for Scholarly Excellence (2009), and the ICDM-
2011 Best Research Paper Award (2011). He
received the B.E. degree from the University of
Science and Technology of China (USTC), China, the M.S. degree from
the National University of Singapore (NUS), Singapore, and the Ph.D.
degree from the University of Minnesota (UMN), USA.
His general area of research is data and knowledge engineering, with
a focus on developing effective and efficient data analysis techniques
for emerging data intensive applications. He has published prolifically
in refereed journals and conference proceedings (3 books, 40+ journal
papers, and 60+ conference papers). He is a co-Editor-in-Chief of Ency-
clopedia of GIS, an Associate Editor of IEEE Transactions on Data and
Knowledge Engineering (TKDE) and the Knowledge and Information
Systems (KAIS) journal. He has served regularly on the organization
and program committees of numerous conferences, including as a
Program Co-Chair of the Industrial and Government Track for the 18th
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining and a Program Co-Chair for the 2013 IEEE International
Conference on Data Mining (ICDM-2013). He is a senior member of the
ACM and IEEE.
Yong Ge received his Ph.D. in Information Tech-
nology from Rutgers, The State University of
New Jersey in 2013, the M.S. degree in Signal
and Information Processing from the University
of Science and Technology of China (USTC)
in 2008, and the B.E. degree in Information
Engineering from Xi’an Jiao Tong University in
2005. He is currently an Assistant Professor at
the University of North Carolina at Charlotte.
His research interests include data mining and
business analytics. He received the ICDM-2011
Best Research Paper Award, Excellence in Academic Research (one
per school) at Rutgers Business School in 2013, and the Dissertation
Fellowship at Rutgers University in 2012. He has published prolifically
in refereed journals and conference proceedings, such as IEEE TKDE,
ACM TOIS, ACM TKDD, ACM TIST, ACM SIGKDD, SIAM SDM, IEEE
ICDM, and ACM RecSys. He has served as Program Committee mem-
bers at the ACM SIGKDD 2013, the International Conference on Web-
Age Information Management 2013, and IEEE ICDM 2013. Also he has
served as a reviewer for numerous journals, including IEEE TKDE, ACM
TIST, KAIS, Information Science, and TSMC-B.
Enhong Chen is currently a Professor and Vice
Dean of School of Computer Science, Vice Di-
rector of the National Engineering Laboratory
for Speech and Language Information Process-
ing of University of Science and Technology of
China (USTC), winner of the National Science
Fund for Distinguished Young Scholars of China.
He received the B.S. degree form Anhui Uni-
versity, Master degree from Hefei University of
Technology and Ph.D degree in computer sci-
ence from USTC.
His research interests include data mining and machine learning, so-
cial network analysis and recommender systems. He has published lots
of papers on refereed journals and conferences, including TKDE, TMC,
KDD, ICDM, NIPS and CIKM. He has served on program committees
of numerous conferences including KDD, ICDM, SDM. He received the
Best Application Paper Award on KDD-2008 and Best Research Paper
Award on ICDM-2011. He is a senior member of the IEEE.

