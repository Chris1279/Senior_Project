Comparative Evaluation of Two Interface Tools in
Performing Visual Analytics Tasks
Dong Hyun Jeong? Tera Marie Green† William Ribarsky? Remco Chang?
?Charlotte Visualization Center †School of Interactive Arts and Technology
UNC Charlotte Simon Fraser University
{dhjeong, ribarsky, rchang}@uncc.edu grepmon@gmail.com
ABSTRACT
In this paper, we performed a comparative evaluation to
show the effectiveness of two interface tools, one a floating
text-based menu (Floating-Menu) and the other a more in-
teractive iconic tool (Interactive-Icon). We evaluated the use
and human performance of both tools within one highly in-
teractive visual analytics system. During task performance
we tracked completion times, task errors, and captured coarse-
grained interactive behaviors.
1. INTRODUCTION
In visual analytics, human interaction and flow of cog-
nition have been noted to be important for problem solv-
ing [1]. Specifically, this previous literature mentioned that
traditional menus cause considerable interruptions to an an-
alyst’s flow, and argued that “visualization design should
avoid, as much as possible, menus or other actions that take
the user outside of the frame of the task. Interactions should
be direct manipulation or closely attached (e.g., with but-
tons in or on the window). This would include pull-down
menus, which require the human to sort through and think
about menu items.”
However, in surveying literature in the HCI community on
the cost and benefits of pulldown menus vs. direct manipu-
lation icons, we find that the distinction between the two is
not nearly as clear. In fact, in a paper by Lim et al. [3], they
reported the results of an evaluation that directly compares
these two systems (menu vs. direct manipulation icons) and
found no time difference on task performance. The claim by
Green et al. and the findings of Lim et al. appear to be con-
tradictory on surface. Since the primary concern in visual
analytics is to help users solving analytical problems (tasks)
efficiently, menu systems are commonly adopted as support-
ing tools. But, the contradictory statements between the
two different menu systems make it difficult to follow for
designers when designing a useful visual analytics system.
The goal of this paper is therefore to perform a user study
to examine the use of these two interface tools involving the
use of a visual analytical tool and determine whether the
claim of Green et al. is valid, or if the finding of Lim et al.
could be extended to complex visual analytical tasks as well.
2. SYSTEM
Within a genomic visualization (called GVis [2]), we tested
two different tools: a Floating-Menu tool and an Interactive-
Icon tool. GVis is an expert visualization system that helps
bioinformaticians to support the visual analysis of large-
scale phylogeny hierarchies populated with the genomic data
of various organisms. It uses a publicly available biologi-
cal database (GenBank) hosted by the National Center for
Biotechnology Information (http://www.ncbi.nlm.nih.gov/)
to visualize the phylogeny hierarchies of organisms and al-
lows the user to quickly browse the hierarchy from the highest-
level (base categorization) down to the level of individual
genome for the desired organism of interest.
Figure 1(a) shows a system overview; phylogenic hierar-
chies are represented in spheres, and each organism is di-
rectly mapped within that sphere. On top of GVis, two
interface tools were designed: Floating-Menu (Figure 1(b))
and Interactive-Icon (Figure 1(c)). To represent 18,000 ci-
tations, an aggregation technique was utilized to make the
visualization both complete and scalable. If an organism is
located in the lower level of the phylogeny, the represented
information is too small to be perceived; aggregation is then
automatically applied to show such information within the
organization of the higher categorizational level.
3. COMPARATIVE STUDY
3.1 Procedure
We conducted a within-subject study to explore which in-
terface was more effective. 31 participants (twelve males
and nineteen females) performed a total six performance
tasks, 3 with each interface. All participants were taking an
introductory psychology course, and received course credit
for participation. Most participants self-reported that they
were unfamiliar with data visualizations.
3.2 Evaluation Results
We present the results of our evaluation based on accu-
racy, speed, difficulty and usefulness, effectiveness, and pref-
erence.
Accuracy: Approximately 54.84% (17.0 ± 7.9) of the
participants answered correctly using Floating-Menu. On
the other hand, when using Interactive-Icon, about 61%
(14.3± 5.0) of them were able to answer correctly. Further-
more, there were two instances in which participants could
not complete the task when using Floating-Menu and three
instances when using Interactive-Icon. By looking at the an-
swers and the captured log-file, some participants (9 partic-
ipants in Floating-Menu and 12 participants in Interactive-
Icon) got close to the goal, but they did not answer cor-
rectly. In such case, we provide a half-point. Based on
the statistical analysis (Repeated Measures ANOVA), we
found that the accuracy difference is not statistically signif-
Figure 1: The system overview (a) showing the biological taxonomy ranking information as a vertical bar
having different color representations and each organism is located in a circular layout having the relevant
color attribute. (b) and (c) show two different interfaces as Floating-Menu and Interactive-Icon. Based on
user’s selection within the interface, it shows a detail information related to each organism group. Least
Square Means of Accuracy (d) and Time spent (e) when solving each task. In (d), the accuracy value close
to 1 indicates the participant finds a correct answer.
icant across the two interfaces with (p = 0.24) and with-
out (p = 0.25) a half point given. Interestingly, we found
that the accuracy difference is statistically significant across
the gender with (F (1, 185) = 8.55, p = .0039) and without
(F (1, 185) = 10.69, p = .0013) a half point given. About
69.4% of male participants correctly solved the given tasks;
whereas only about 50.4% of female participants provided
correct answers.
Speed: We assume that the user might spend less time
when solving the given tasks with an interactive interface
tool. However, we found no statistically significant differ-
ence (p < 0.25) on time-spent.
By measuring least square means about the accuracy (Fig-
ure 1(d)) and the time spent (Figure 1(e)), participants
spent more time when solving more difficult task questions
(difficulty of each task: task 1 < task 2 < task 3). Based on
Pearson’s Correlation Coefficient measure, there is a trend
between the time spent and the difficulty of the task (r =
.47, p < .0001). Although it is not statistically significant,
there is a negative correlation between the accuracy and the
difficulty of the task (r = ?.12, p = 0.09). The partici-
pants spent less time with Interactive-Icon for solving the
easy task (task 1), the accuracy is lower than with Floating-
Menu. However, when solving the difficult task (task 3),
the accuracy was slightly, but not significantly higher with
Interactive-Icon even if they spent significantly more time.
Easiness & helpfulness (post-task questionnaire):
Participants are requested to report the easiness and help-
fulness of the interface in solving the task. About 60%
(18.6 ± 0.5 for each task) and 43% (13.3 ± 4.0 for each
task) of the participants reported all 3 tasks to be “easy”
or “very easy” when using Floating-Menu and Interactive-
Icon, respectively. Also about 74% (23 ± 3.6 for each task)
of the participants identified Floating-Menu to be “helpful”
or“very helpful”in solving the tasks, and about 65% (20±3.6
for each task) of the participants found Interactive-Icon to
be “helpful” or “very helpful.”
Learnability (post-application questionnaire): The
post-application questionnaire conducted right after the eval-
uation of each interface tool. Of particular significance are
the questions asking the participants how easy was the inter-
face to use and how easy was the interface to learn. About
67% and 51% of the participants rated that Floating-Menu
and Interactive-Icon were easy to use (“very easy”or“easy”),
respectively. Since all participants were not familiar with
interactive visual interface, they felt simple selection mecha-
nism in Floating-Menu was much easier to learn than Interactive-
Icon. These might be because user’s familiarity is quite re-
lated to determining ease of learning and ease of use on each
interface.
Preference (post-study questionnaire): In regards
to the participants’ preferences, there is no significant dif-
ference. Based on the description of the pros and cons, we
found that several participants pointed out the strength of
Interactive-Icon as being “interactive”.
4. DISCUSSION AND FUTUREWORK
Our findings demonstrate that the two interface tools,
Floating-Menu and Interactive-Icon, perform similarly both
quantitatively and qualitatively.
In comparing Green et al.’s claim that menus cause con-
siderable interruption to the analysis process, and Lim et
al.’s reports that menus and direct manipulation icons are
comparable in performance, our findings seem to support
the latter claims. It would appear that although the evalu-
ation conducted by Lim et al. uses only trivial tasks, their
findings may be generalizable in some ways to complex anal-
ysis processes using visual analytical tools. However, there
are other factors that need to be considered in evaluating
these contradictory claims. These experiments, as well as
the considerations and analysis they engendered in this pa-
per have set the stage for significant new work. Based on
this analysis, we and others can now undertake deeper and
more careful experiments on the relationships between the
use of interactive tools during engaged cognitive flow and
reasoning processes. This is a central research issue for vi-
sual analytics.
5. REFERENCES
[1] T. Green, W. Ribarsky, and B. Fisher. Visual analytics
for complex concepts using a human cognition model.
In VAST ’08, pages 91–98, 2008.
[2] J. Hong, D. H. Jeong, C. D. Shaw, W. Ribarsky,
M. Borodovsky, and C. G. Song. Gvis: A scalable
visualization framework for genomic data. In EuroVis
’05, pages 191–198, 2005.
[3] K. H. Lim, I. Benbasat, and P. A. Todd. An
experimental investigation of the interactive effects of
interface style, instructions, and task familiarity on user
performance. ACM Trans. Comput.-Hum. Interact.,
3(1):1–37, 1996.

