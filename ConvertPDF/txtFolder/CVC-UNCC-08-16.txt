Integrating Multi-Modal Content Analysis and Hyperbolic Visualization for
Large-Scale News Video Retrieval and Exploration
H. Luoa, J. Fan1b, S. Satohc, J. Yangb, W. Ribarskyb
aSoftware Engineering Institute, East China Normal University, Shanghai, China
bDepartment of Computer Science, UNC-Charlotte, Charlotte, USA
cNational Institute of Informatics, Tokyo 101-8430, Japan
Abstract
In this paper, we have developed a novel scheme to achieve more effective analysis, retrieval and exploration of large-scale news video collections
by performing multi-modal video content analysis and synchronization. First, automatic keyword extraction is performed on news closed
captions and audio channels to detect the most interesting news topics (i.e., keywords for news topic interpretation), and the associations among
these news topics (i.e., contextual relationships among the news topics) are further determined according to their co-occurrence probabilities.
Second, visual semantic items, such as human faces, text captions, video concepts, are extracted automatically by using our semantic video
analysis techniques. The news topics are automatically synchronized with the most relevant visual semantic items. In addition, an interestingness
weight is assigned for each news topic to characterize its importance. Finally, a novel hyperbolic visualization scheme is incorporated to
visualize large-scale news topics according to their associations and interestingness. With a better global overview of large-scale news video
collections, users can specify their queries more precisely and explore large-scale news video collections interactively. Our experiments on
large-scale news video collections have provided very positive results.
Key words: Multi-Modal Content Analysis, Interestingness Assignment, Association Determination, Hyperbolic Visualization.
1. Introduction
The broadcast news videos have extensive influence and
carry abundant information, different organizations and indi-
viduals are utilizing them for different purposes. For example,
the government can boost up the morale by publishing posi-
tive reports. On the other hand, the terrorists can also use it to
display their announcement (e.g., Al Jazeera), formulate their
plans, raise funds, and spread propaganda. By watching and an-
alyzing the international news reports, the intelligence agents
can translate the raw news videos into useful information and
the private investors can also evaluate the political, economic,
and financial status for making good decisions. Unfortunately,
watching large-scale news video collections could be very te-
dious. Due to the large number of broadcast channels, it is too
expensive and time-consuming to hire people to do manual pro-
cess, and such manual process of large-scale news video col-
lections may also delay our response for critical news events.
Therefore, there is a growing interest of developing more effec-
tive techniques for automatic news video analysis and explo-
1 Corresponding author. Tel.: 704-687-8556, FAX: 704-687-3516, E-mail:
jfan@uncc.edu (J. Fan)
ration of large-scale news video collections. In addition, sup-
porting automatic news video analysis and exploration is able
to acquaint general audiences with a better global overview of
large-scale news video collections, so that they can also harvest
our research achievements. Unfortunately, most existing tools
for automatic news video analysis and exploration still suffer
from the following challenging problems.
The first problem is how to automatically extract the video
semantics from news video clips and bridge the semantic gap
[7, 17, 40] between the low-level visual features and the high-
level human interpretation of video semantics. Most existing
systems can only support automatic extraction of low-level vi-
sual features and search video clips via similarity matching ac-
cording to their low-level visual features [1, 2, 5, 30, 33, 41, 42,
44, 51, 52]. On the other hand, most users may not be familiar
with the low-level visual features and they can only express their
information needs via high-level video concepts (i.e., keywords
for video concept interpretation) [16, 17]. Due to huge diversi-
ties of visual properties and semantics in news videos, most ex-
isting tools for semantic video classification cannot directly be
extended to achieve automatic analysis and exploration of large-
scale news video collections [1, 2, 5, 9, 33, 41, 42, 44, 51, 52].
The second problem is how to extract most significant news
Preprint submitted to Elsevier 14 April 2008
 
S em an tic  
V ideo  A na ly s is  
In te re s tin gne ss  
W eigh ting  
S em an tic  
In te rp re ta tion   V isu a liz a tion  
K now led g e  
In te rp re ta tio n   
F eedb ack   
Fig. 1. The workflow of our proposed framework for analysis and exploration
of large-scale news video collections.
topics from large-scale news video collections. This problem
is becoming more critical because of the following reasons:
(a) The amount of news topics could be very large because
there are so many broadcast channels; (b) Different news topics
may have different importance; (c) Different users may have
different interestingness on the same news topic and there is
no existing approach for user interestingness modeling. Thus
there is an interest gap [47] between the available collections
of news topics and the user’s real information needs.
The third problem is how to present the significant news top-
ics to the users efficiently and effectively. Large-scale collec-
tions of news videos may carry huge amount of information.
However, it is very slow to exchange information between the
user and the computer. Generally, a person can accept informa-
tion at the speed of tens of bits per second. But our database
has 10 GB to 1TB or larger scale. As a result, there is an in-
teraction gap between the computer and the user.
Based on the above observations, we have developed a new
framework with the workflow given in Figure 1. Firstly, vi-
sual semantic items for video content interpretation are auto-
matically extracted from raw video clips by using our seman-
tic video analysis technique. The keywords (e.g., text semantic
items) for news topic interpretation and their associations (e.g.,
contextual relationships between the news topics) are extracted
from the closed captions and audio channels, and they are fur-
ther synchronized with the most relevant visual semantic items.
Secondly, the interestingness measurements (weights) for these
multi-modal (i.e., visual, auditory, and textual) semantic items
are assigned simultaneously via statistical analysis. Finally, a
novel hyperbolic visualization framework is incorporated to vi-
sualize large-scale news relation network (i.e., news topics and
their associations) and support interactive news exploration, so
that users can have better understanding of large-scale collec-
tions of news videos and make better query decisions. In addi-
tion, the users can also communicate with our system by pro-
viding their feedbacks, and such feedbacks can further be in-
tegrated to improve our algorithms for semantic video analysis
and interestingness assignment.
Each step of the proposed framework bridge one of the three
gaps as described above. Our proposed scheme is not a sim-
ple aggregation of the related techniques. It optimizes all steps
toward the final goal. Therefore, the techniques used in our
scheme may not be the best one in the relevant area, but it is
the best suitable one for our proposed scheme. Firstly, because
the vision is the fastest way to accept information for human
beings, we adopt visualization to bring the interaction between
the users and our system. The visualization technique can al-
low our system to present more information at the same time
to the users than traditional techniques adopted by other re-
trieval or exploration systems, such as classified exploration and
keyword-based search. Secondly, to fully utilize the capability
of the visualization technique, we adopt a very small seman-
tic unit (compared to the units adopted in other video seman-
tic analysis and retrieval systems) to characterize the semantics
and interestingness of the video news reports. On the one hand,
it is more easy to detect and evaluate small semantic items,
thus our algorithms for semantic video analysis and interest-
ingness weighting can have higher performance. On the other
hand, more accurate evaluation of the interestingness weight-
ing enables more effective visualization. As a result, our pro-
posed scheme may have better performance than aggregating
the most sophisticated techniques in the relevant areas.
In this paper, we have developed multiple algorithms to ad-
dress the following problems: (1) How to extract semantic items
from large-scale news video collections that may have huge
diversities of visual properties and semantics? (2) How to ex-
tract news topics and their associations? (3) How to assign the
interestingness with the news topics and semantic items auto-
matically? (4) How to achieve synchronization between multi-
modal news video contents? (5) How to visualize large-scale
relation network (i.e., news topics and their associations) in a
limited-size scene?
This paper is organized as follows: In section 2, we briefly
review the most relevant works. In section 3, we introduce our
algorithm for news topic detection and relation network gener-
ation. Section 4 describes our scheme for user interestingness
modeling. Section 5 introduces our semantic video analysis and
multi-modal video content synchronization algorithm. In sec-
tion 6, we introduce our work on hyperbolic visualization of
large-scale relation network. Finally, we conclude in Section 8.
2. Related Works
In the past, researchers have proposed some interesting ap-
proaches for bridging these gaps (i.e., semantic gap, interest
gap and interaction gap). In this section, we give a brief review
for some of these existing algorithms which are most relevant
to our proposed work.
2.1. Bridging Semantic Gap
Semantic video classification is one promising solution to
bridge the semantic gap [1, 2, 4, 5, 12, 14, 33, 41, 42, 44, 51, 52],
but its performance largely depends on two inter-related issues:
(1) suitable algorithms for video content representation and
feature extraction; (2) effective algorithms for video classifier
training.
To address the first issue, many approaches have been pro-
posed for video content representation and feature extraction,
and most existing approaches can be classified into three cat-
egories: (1) shot-based approaches that extract the global vi-
sual features from whole video shots [19, 36]; (2) region-
based approaches that extract the local visual features from mo-
tion, color, or texture consistent homogeneous video regions
[10, 18, 28]; and (3) object-based approaches that extract the
representative visual features from semantic-sensitive video ob-
2
jects [24]. To enhance the power of the visual features on dis-
criminating various video concepts, the underlying video pat-
terns for feature extraction should be able to capture the inter-
mediate video semantics at the object level effectively and effi-
ciently (i.e., semantics for interpreting the real world physical
objects in a video clip). Using the segmentation of real world
physical objects (i.e., video objects) for feature extraction can
significantly enhance the ability of the visual features on dis-
criminating various video concepts. Unfortunately, automatic
detection of large amounts of semantic video objects with di-
verse perceptual properties is still an open problem in com-
puter vision. On the other hand, both the shot-based and the
region-based approaches are easy for implementation, but their
visual features may not be representative and cannot be used to
discriminate various video concepts effectively. Consequently,
there is an urgent need to develop new framework for video
content representation and feature extraction, which is able to
take the advantages for all these three existing approaches and
avoid their shortcomings.
To address the second issue, robust video classifier training
techniques are needed to model both the inter-concept varia-
tions and intra-concept similarity effectively. Two approaches
are widely used to train video classifiers [1, 2, 4, 5, 12, 14,
20, 27, 33, 41, 42, 44, 51, 52]: (a) GMM-based approach uses
Gaussian mixture models to approximate the underlying distri-
butions of video data; (b) SVM-based approach uses support
vector machines to maximize the margin between the posi-
tive samples and the negative samples. As a generative model,
the major advantage of the GMM-based approach is that prior
knowledge can be effectively incorporated into training suitable
concept models for accurate video classification and annotation
by predefining part of the model structure. Due to the diver-
sity and richness of video contents, GMM models for video se-
mantics interpretation may contain hundreds of parameters in
a high-dimensional feature space, and thus large-scale labeled
videos are needed for accurate classifier training. In addition,
there may be mismatch between Gaussian functions and the
real distributions of video data. On the other hand, the SVM-
based approach enables more effective classifier training with
smaller generalization error rate in high-dimensional feature
space. However, searching the optimal model parameters (i.e.,
SVM parameters) is computationally expensive, and its perfor-
mance is very sensitive to choices of kernel functions. Further-
more, automatic kernel function selection heavily depends on
the implicit geometric property of the video data in the high-
dimensional feature space. Because the high-dimensional fea-
ture space may be heterogeneous, it is difficult to select one
common kernel function that can effectively characterize the
implicit geometric properties of the video data. Another short-
coming of the SVM-based approach is that its training com-
plexity depends on the number of training videos, and the prior
knowledge (i.e., the correlations between video concepts) is not
incorporated into SVM video classifier training.
The Informedia Digital Video Library project at CMU has
achieved significant progresses on analyzing, indexing and
searching of large-scale news video collections. A detailed
survey can be found in [21]. Several applications have been
reported, such as keyword-based video retrieval and query
results visualization. The DELOS project also has significant
progress on multiple areas including information access and
personalization[11], object detection[3] and visualization[8].
Our proposed work has significant differences from these
existing works: (a) Rather than performing semantic video
classification to achieve automatic news video content under-
standing, multi-modal content analysis results from multiple
information sources are synchronized and integrated for in-
dexing and exploring large-scale news video collections. Even
though many video classification algorithms have been pro-
posed [1, 2, 4, 5, 12, 14, 33, 41, 42, 44, 51, 52], they cannot
be used to achieve reliable classification of large-scale news
video collections because of their huge diversities of visual
properties and semantics. Most existing techniques for seman-
tic video classification can perfectly work on same specific
video domains with strong constraints of video contents. How-
ever, none of them can effectively handle the huge diversity
of visual properties and semantics in the news videos. (b) An
interestingness score is automatically assigned to each news
topic via statistical analysis, and such interestingness scores
are further used to decide the importance of the relevant news
topics for filtering less interesting news topics. (c) The associ-
ations among the news topics are extracted for achieving more
effective visualization and interactive exploration of large-
scale news video collections. (d) A hyperbolic visualization
tool is incorporated to acquaint the users with a better global
overview of large-scale news video collections, so that they
can explore large-scale news video collections interactively.
2.2. Bridging Interest Gap
To bridge the interest gap, visualization is widely used to
help the users explore large amounts of information and find
interesting parts interactively. In-spire [50] has been developed
for visualizing and exploring large-scale text document collec-
tions, where statistics of news reports [31] is put on a world
map to inform the audiences of the “hotness” of regions and
the relations among the regions. TimeMine [45] is proposed to
detect the most important reports and organize them through
timeline with statistical models of word usage. Another system,
called newsmap [49], can organize news topics from Google
news on a rectangle, where each news story covers a visualiza-
tion space that is proportional to the number of related news
pages reported by Google. News titles are drawn in the corre-
sponding visualization space allocated to relevant news topic.
ThemeRiver [22] and ThemeView [23] can visualize a large
collection of documents with keywords or themes. ThemeRiver
and ThemeView can intuitively represent the distribution struc-
ture of themes and keywords in the collections.
Rather than recommending the most interesting news topics
to the users, all of these existing news visualization systems
prefer to disclose all the available information to the users, and
thus the users have to dig out the interesting information by
themselves. When large-scale news collections come into view,
such available information could be very large and displaying
3
all of them to the users may mislead them. To address this prob-
lem, some of these existing news visualization systems also dis-
close different distribution structures of large-scale news collec-
tions, but such distribution structures may not make any sense
to the users because there is an interest gap between the dis-
tribution structures and the user’s real information needs [47].
Therefore, it is very attractive to incorporate the interesting-
ness of news topics for achieving more effective visualization
and exploration of large-scale news video collections. Several
researchers use predefined ontology to assistant visual content
analysis and retrieval [13, 15, 46]. However, such pre-defined
ontology is unacceptable for news content representation be-
cause the news content is highly dynamic. How to extract the
semantic structure automatically from large-scale news video
collections is still an open problem.
2.3. Bridging Interaction Gap
As addressed above, the vision is the fastest way for human
beings to accept information. Thus supporting news video vi-
sualization is one potential solution for exploring large-scale
news video collections. Therefore, visualization techniques are
potential solution. To fully utilize user’s vision capability, the
adopted visualization technique must adapt to the human be-
ing’s vision property. It is reported [26, 48] that a human be-
ing will focus on the details of a single point but still check
the global context at the same time. Because the hyperbolic
visualization technique [26, 48] implements the fish-eye effect
with a uniform framework, which is very suitable for the goal
of our system. However, the hyperbolic visualization technique
is only a single layer exploration technique for a specific for-
mat of information. The information carried in large-scale news
video collections may need hierarchical interactive exploration.
Therefore, how to transform the data upon user input to im-
plement user-adaptive hierarchical exploration is still an open
problem.
In [29], a preliminary framework is proposed to resolve the
above problems. This paper is the extension of [29]. In this pa-
per, we developed the text term extraction and relation network
generation algorithm, introduced a new visualization approach
and proposed an algorithm to perform user-adaptive visualiza-
tion. These extensions greatly improve the overall performance
of our previous work [29].
3. Text Term Extraction and Relation Network Generation
For news programs, the closed captions can provide abundant
information and such information can be used to detect news
topics of interest with high accuracy. Rather than performing
semantic video classification to obtain the news topics [1, 2,
5, 20, 27, 33, 41, 42, 44, 51, 52] (i.e., which can extract only
a limited number of semantic concepts via multi-modal visual
features), we have taken the advantage of cross-media to clarify
the video contents and achieve automatic news topic detection
from news closed captions. To do this, the closed captions are
first segmented into sentences, and the text sentence is further
segmented into keywords.
In news videos, some special text sentences, such as “some-
body, CNN, somewhere” and “ABC’s somebody reports from
somewhere”, need to be processed separately. The names for
news reporters in those text sentences are generally not the
content of the news report. Therefore, they are not appropri-
ate for news semantics interpretation and should be removed.
Because there may have clear and fixed patterns for these sen-
tences, we have designed a context-free syntax parser to detect
and mark this particular information. By incorporating 10-15
syntax rules, the parser can detect and mark such specific sen-
tences in high accuracy.
Most named entity detectors may fail in processing all-capital
strings because initial capitalization is very important to achieve
accurate named entity recognition. One way to resolve this
problem is to train a detector with ground truth from the closed
caption text. However, it’s very expensive to obtain the manu-
ally marked text material. Because English has relatively strict
grammar, it’s possible to parse the sentences and recover the
most capital information by using part-of-speech (POS) [39]
and lemma information. We use TreeTagger [39] to perform the
part-of-speech tagging. Capital information can be recovered
automatically by using the TreeTagger parsing results.
After such specific sentences are marked and capital infor-
mation is recovered, an open source text analysis package Ling-
Pipe [25] is used to perform the named entity detection and
resolve co-reference of the named entities. The named entities
referring to the same entity are normalized to a most repre-
sentative format to enable statistical analysis, where the news
model of LingPipe is used and all the parameters are set to
default value.
Finally, the normalized results are parsed again by TreeTag-
ger to extract the POS information and resolve the words to
their original formats. For example, TreeTagger can resolve
“better” to “well” or “good” according to its POS tag. We do
not adopt the stemming technique because it may output un-
readable words and resolve different words to the same stem.
By using the POS tag to resolve the words to their original for-
mats, this problem can be resolved. In addition, the POS tag
can be used to remove words without real meanings, such as
adverbs and prepositions. Most stop words can be removed by
POS tag. All these detected keywords are treated as a basic
vocabulary for interpreting the relevant news topics.
The relations among different news topics are also very im-
portant to enable more effective retrieval and exploration of
large-scale news video collections. Because the news topics
have already been extracted, the relations (associations) among
these news topics can further be extracted according to their co-
occurrence probabilities. Based on this observation, a weighted
news topic relation network is used for knowledge interpreta-
tion. Based on this observations, we use a weighted news topic
relation network as the knowledge interpretation. The network
uses news topics (i.e., keywords and keyframes) as nodes and
their relations as edges, and the edges are weighted according
to their interestingness for the users. If we use D to represent
the collection of news videos of interest and KD to represent
4
Fig. 2. Links of the semantic item “test” disclose details of the event and
response of the international community during the North Korean nuclear
weapon test.
Fig. 3. North Korean nuclear weapon test event in the global news reports
context.
the knowledge interpretation of D (i.e., the weighted relation
network), KD can be represent as:
KD = {(ki = (sa, sb) , wU (ki)) | 1 ? i ? N} (1)
where ki is a relation (i.e., a pair of news topics sa and sb),
U is the user who is using the system, and wU (ki) is the in-
terestingness weight of ki based on U ’s preference. A pair of
news topics sa and sb is defined as a relation when they oc-
cur in a closed caption or automatic speech recognition (ASR)
script sentence simultaneously. By collecting all these relations
together, the semantics of the large-scale collections of news
videos can be represented. Examples of the relation network
are given in Figure 2 and Figure 3.
Even though the relation network is a good semantic rep-
resentation of large-scale collections of news videos, not all
of these relations are interesting for the users. To resolve this
problem, we also compute an interestingness weight wU (ki)
for each knowledge item ki. Our knowledge interpretation ex-
traction algorithm will extract the interestingness weights for
the knowledge items. The algorithm is introduced in the next
section.
4. User Interestingness Modeling
The interestingness of the news topics and their relations
must be quantified to implement our visualization system, but
most existing visualization techniques [22, 49] use the raw fre-
quency or probability to organize the visualization, and the raw
frequency or probability may not indicate user’s information
need. For example, “Bush” is a keyword with very high fre-
quency in 2006, but it may not be interesting because it is al-
ready well known. As the same reason, the relation between
“President” and “Bush” is uninteresting for most users because
it is already known. In the scenario of news browsing and re-
trieval applications, a user may be interested in only the in-
formation that he or she did not know before (e.g. the “really
unexpected” information).
To resolve this problem, the interestingness of the news top-
ics and their relations must be assigned for visualizing large-
scale news video collections. Ideally, the interestingness is re-
lated to the user’s preference and their prior knowledge, but it
is very difficult to gather user preference information. Because
the users may learn information from different sources that that
our system may not know, such as friends. In addition, it’s even
more difficult to have an accurate knowledge model of a spe-
cific user in the foreseeable future. Thus a general interesting-
ness measurement that is effective for most users is proposed
in this paper.
Google has implemented a great search engine based on the
PageRank [35] technique. The PageRank technique ranks the
web pages according to the provider behavior (e.g. the links of
web pages). Even though Google has no user preference data,
the PageRank technique can still give reasonable ranks of web
pages for most users. Based on this observation, the provider
behavior is valuable information for quantifying the interest-
ingness. More importantly, it’s possible for us to gather the
provider behavior information. Thus we can quantify the inter-
estingness of news topics by using the statistical information
extracted from many TV channels. To quantify the interesting-
ness of news topics with the provider behavior, we assume that
the audiences may have higher probability to know a piece of
information if it is repeated more frequently on TV programs.
Thus the past news collection can be used to extract a general
knowledge model of the user’s knowledge. We use a probabil-
ity distribution to represent the knowledge model:
G = {g (x) |x ? S} (2)
where x is the given news topic or relation, S is the set of all
the news topics and their relations, and g (x) is the probability
of the news topic or relation x. The general knowledge model
can be used as a predictor. If a news topic or relation can be
predicted completely, then it may not be interesting at all for
the users. Only those news topics or relations that cannot be
predicted well may be interesting for the users. According to
information theory, the unpredictability of a message (i.e. news
5
topics or their relations in our system) can be quantified by the
information it carries. To quantify the amount of the information
a news topic or relation carries, the local probability model
of news topics or relations of news reports for a specific time
interval of interest is defined as:
L (t) = {lt (x) |x ? St ? S} (3)
where t is the specific time interval of interest, such as one
specific day, St is the set of all the news topics or their relations
in the specific time interval t and lt (x) is the probability of
the given news topic or relation x in t. The difference between
the local probability model L (t) and the general knowledge
model G is able to tell us how much information we can obtain
by knowing L (t). Because both the local probability model
L (t) and the general knowledge model G are distributions,
the Kullback-Leibler divergence is used to characterize their
difference:
D (L (t) ? G) =
?
x?St
lt (x) log
lt (x)
g (x)
(4)
The distance function D (L (t) ? G) is able to characterize
the difference between L (t) and G, but we also need to evalu-
ate the information carried by each news topic or relation x ?
St. By examining Eq. (4), one can observe that D (L (t) ? G)
is the weighted sum of a set of components, and each compo-
nent is only related to one single news topic x. Therefore, the
contribution for one certain news topic x ? St can be obtained
by the relevant component of D (L (t) ? G) in Eq. (4). Based
on this observation, we can quantify the interestingness of x as:
wrt (x) = lt (x) log
lt (x)
g (x)
(5)
From Eq. (5), one can observe that the interestingness mea-
surement wrt (x) depends on two factors: lt (x) and
lt(x)
g(x) . The
first factor lt (x) characterizes the probability of news topic
x in local probability model L (t) and the second factor lt(x)g(x)
characterizes the probability difference of x in between L (t)
andG (i.e., unpredictability). Because the purpose of Kullback-
Leibler divergence is to measure the overall difference of L (t)
and G, as a result the unpredictability factor lt(x)g(x) must be
scaled by lt (x) in Eq. (4) so that D (L (t) ? G) is adapt to the
local distribution L (t). However, our purpose is to measure
the interestingness of individual news topic x. Therefore, the
scale lt (x) is irrelevant to our purpose. Consequently, only the
unpredictability factor lt(x)g(x) should be used to characterize the
interestingness measurement:
wdt (x) =
lt (x)
g (x)
(6)
Because multiple media channels (audio, video and closed
caption text) are involved in the visualization, wdt (x) in Eq. (6)
needs to be normalized to simplify the multi-modal data fusion:
wt (x) =
wdt (x)
max
x?St
{
wdt (x)
} (7)
Eq. (7) normalizes the interestingness weights to [0, 1]. The
normalized weights are preferable to fuse with other factors. In
our system, we use Eq. (7) to compute wU (ki), i.e., wU (ki) =
wt (ki). And Eq. (7) is a general weighting algorithm that can
be applied to weigh news topics, relations and other visual
properties such as video semantic concepts. Therefore, we use
Eq. (7) to compute not only wU (ki) but also interestingness
weights of other items.
With the above algorithm for assigning the weights for the
news topics and their relations, our system is able to extract the
interesting news topics and suppress less interesting news top-
ics. Because only statistical important semantic items can have
high interestingness measurement, the random error of seman-
tic analysis, which outputs a number of different incorrect se-
mantic items, can be filtered out automatically and more robust
results can be achieved.
To enable more efficient visualization of large-scale news
video collections, special visual features must be considered,
or else the results may not be visually important. There are
multiple types of visual features that may be important. Some
types of visual features can be processed by using Eq. (7), such
as the people and the semantic concepts of news video shots.
Other types of visual features may not be characterized by using
the same statistical analysis algorithms as described above, such
as video production rules. Based on this understanding, we
have developed a series of semantic video analysis algorithm
to extract the needed video semantics. These algorithms are
introduced in the next section.
5. Semantic Video Analysis
Even though semantic video analysis and understanding are
still very challenging, supporting semantic video analysis plays
an important role in enabling more efficient retrieval and ex-
ploration of large-scale news video collections. Based on this
observation, we have developed multiple solutions to automati-
cally detect multi-modal semantic items (i.e., video, audio, text)
and extract the representative visual features for video content
representation. In addition, the interestingness weights for these
semantic items are assigned automatically via our statistical
video analysis algorithm.
5.1. Statistical Property Analysis of Video Shots
The video shots are the basic units for video content repre-
sentation, and thus they can be treated as one of the semantic
items for automatic weight assignment. However, unlike the
keywords in text documents, the re-appearance of video shots
cannot be detected automatically via simple comparison of their
visual properties. Thus new techniques are desired for detect-
ing the re-appearance of video shots in news videos [29], so
that we can assign the importance weights of video shots au-
tomatically.
One certain video shot may be repeated multiple times be-
cause of the following reasons: (a) video shots for the anchors
may repeat multiple times in the same news program; (b) video
6
shots for the participants of an interview may appear multiple
times in the same news program; (c) video shots for interpret-
ing the important news may appear in both the news summary
at the beginning and the detailed report later in the same pro-
gram; (d) video shots for the important news may appear in
different news programs of the same channel (at different time
periods) or different TV channels (at different time or same
time). The last two situations of video shot re-appearance in-
dicate the importance for the corresponding video shots. Nev-
ertheless, the first two situations of video shot re-appearance
may not indicate that the corresponding video shots are im-
portant. To quantify the effect of above four rules, the intra-
program re-appearance number rintra (i) and inter-program re-
appearance number rinter (i) for each video shot are computed.
The two re-appearance numbers rinter (i) and rintra (i) for
most video shots are equal to 1 because they are not repeated.
Obviously, some video shots may have these two numbers big-
ger than 1 and different re-appearance patterns (i.e., different
re-appearance situations) may provide different semantics and
different weights should be assigned. Based on these under-
standings, the weights for different re-appearance numbers of
video shots are approximated by using a bell shaped curve:
wintra (i) = e?
(
rintra(i)?%intra
%intra
)2
2
winter (i) = e?
(
rinter(i)?%inter
%inter
)2
2
(8)
where %intra and %inter are the parameters. For intra-program
repetition of video shots, Rules (a), (b) and (c) apply. How-
ever, the shots satisfying Rules (a) and (b) are not very attrac-
tive while the shots satisfying Rule (c) may be attractive. To
discriminate the re-appearance pattern of Rule (c) with that of
Rules (a) and (b), %intra = 2 is adopted in our experiments. As
a result, anchor and interview shots re-appearing many times
are suppressed because they are not “visually” important. On
the other hand, news shots repeated in both the summary and
the detailed report are emphasized, because their intra-program
re-appearance number is exactly 2. For inter-program repeti-
tion, we found via experiments that little shot is repeated more
than 5 times in different programs. In addition, if a news re-
port is repeated channel by channel and program by program, it
quickly becomes well known and therefore no longer “news”.
Therefore, we set %inter = 5 in our experiments.
5.2. Video Objects Detection
For news videos, video objects, such as text areas and hu-
man faces, may provide important clues about news stories
of interest. Text lines and human faces in news videos can
be detected automatically by using suitable computer vision
techniques [29]. Obviously, these automatic detection functions
may fail in some cases. Thus the results that are detected by
using a single video frame may not be reliable. To address this
problem, the detection results on all the video frames within
the same video shots are integrated and the corresponding con-
fidence maps for the detection results are calculated. As shown
(a) Detected text lines (b) Confidence map of text
(c) Detected faces (d) Confidence map of faces
Fig. 4. Automatic text and face detection results
in Figure 4, such confidence maps can provide valuable infor-
mation for evaluating our detection results.
The confidence region is generated by transforming the rel-
evant confidences for our detection results into a binary image
via thresholding. The threshold for generating the confidence
region of text is set to the recall of the object detection algo-
rithm. The reason of using recall as the threshold is that our
object detection algorithm is tuned with high precision. There-
fore, object regions will have confidence value higher or equal
to the recall. Obviously, the size ratio between the confidence
region and the size of video frames provides some valuable
information for weight assignment. Therefore the size ratios
for text and human faces regions are obtained, ?text (i) and
?face (i). However, the ratios cannot be directly used as the
weight of the video shot. News editors may emphasize a per-
son by putting a close-up, resulting in a large ?face (i). But a
small face generally does not mean the person is unimportant,
it only means the shot is an event shot. The text object is ana-
log to the face object. As a result, the ratios must be converted
by using a lower-bounded curve:
warea (i) =
1
1 + e?
max{?(i)??, 0}
?
(9)
where ? is lower-bound ratio that news editors adopt for em-
phasizing information. For the face object, it generally equals
the average ratio in anchor shots. For the text object, it generally
equals the average ratio in shots without extra open captions
(e.g., only with open captions overlayed on every shot). When
? is less than ?, we cannot tell whether the news editor wants to
emphasize the object or not. Therefore, the curve of Eq. (9) as-
signs the same weight to all these shots. For shots with ? larger
than ?, ? controls the conversion from ? to the weight. To de-
cide appropriate values for ? and ?, we compute the ?text (i)
and ?face (i) of all shots on a 10-hour news video database.
We found that the average ?face (i) for anchor shots is close
to 0.01 and average ?text (i) for shots without extra open cap-
tions is close to 0.05. Therefore, we adopt ?text = 0.05 and
7
?face = 0.01. To compute ?, we select the control point of the
conversion at 0.8, e.g., if a shot has a ? (i) value that is larger
than 80% of the shots (only counting shots with ? (i) ? ?),
then it is assigned the weight 0.8. According to experimental
data, ?text = 0.1593 and ?face = 0.04096. For a given video
shot, the importance weight for human face area wfaceArea (i)
and the importance weight for text area wtextArea (i) can be
determined by:
wtestArea (i) =
1
1+e?
max{?text(i)??text, 0}
?text
wfaceArea (i) =
1
1+e
?max{?face(i)??face, 0}?face
(10)
By performing face clustering [38], face objects can be clus-
tered into several groups. As a result, the human objects can be
identified and be treated as one of the semantic items for weight
assignment. The face object is similar to text term. Therefore,
they can be weighted by using news topics weighting algorithm
as Eq. (7), which is introduced in Section 4. In addition, there
may be more than one human object in a video shots. We just
pick up the one with the highest weight as the representative
for the video shot:
whuman (i) = max
{
max
x?HUMAN(i)
{wt (x)} , 0.5
}
(11)
where HUMAN (i) is the set of human objects of shot i.
The first max {•} lower-bounds the human weight at 0.5 for
the same reason discussed above. w¯t (•) is the news topics
weighting algorithm of Eq. (7). The second max {•} ensures
our algorithm to detect the most important person in the shot
and ignore unimportant people.
5.3. Semantic Video Classification
The video concepts associated with the video shots can pro-
vide valuable information to enable more effective visualization
and retrieval of large-scale news video collections, and seman-
tic video classification is one promising solution to detect such
video concepts. To detect the video concepts automatically, we
have adopted our previous work reported in [17].
Two types of information about video concepts can be used
for weight assignment. First, each video concept has an intrin-
sic importance. For example, a shot with a person reading an
announcement is more important than a shot with a journal-
ist introducing background information. The importance of the
video concepts, wc (C (i)), is determined by user study. Re-
sults are in Table 1. Where C (i) is the video concept in the
video shot i.
Second, the video concept can be treated as a text term. As a
result, it can also be weighted as news topics by Eq. (7), which
is introduced in Section 4. Finally, the weight for the given
video concept is determined by:
wconcept (i) = wc (C (i))× wt (C (i)) (12)
where w¯t (•) is the news topics weighting algorithm of Eq. (7).
Table 1
Video Concept Importance
Concept C (i) wc (C (i)) Concept wc (C (i))
Announcement 0.9 Report 0.3
Sports 0.5 Weather 0.5
Gathered People 1 Unknown 0.8
5.4. Semantic Visual Weights Fusion
Our purpose of weighting is to detect the existence of some
visual properties and emphasize those shots with interesting
visual properties. The existence of one visual property may be
indicated by different visual patterns. For example, the repeat
property may be represented by wintra or winter. To ensure we
detect the existence of interesting visual properties and do not
be misled by missing some visual patterns, we first use max
operation to fuse weights for the same visual property:
wrepeat (i) = max {wintra (i) , winter (i)}
wobject (i) = max {wfaceArea (i) , wtextArea (i)}
wsemantics (i) = max {whuman (i) , wconcept (i)}
(13)
They are all derived from visual characteristics of video shots.
Where wrepeat is the weight reflecting repetition of the shot,
wobject reflecting prominent objects in the shot, and wsemantics
reflecting semantic categories. Consequently, the overall visual
importance weight for a given video shot is determined by the
geometric average of above three weights:
wvideo (i) = 3
?
wrepeat (i)× wobject (i)× wsemantics (i) (14)
5.5. Multi-Modal News Content Synchronization and
Decision Fusion
For news programs, the closed captions have good matching
with the relevant audios. Therefore, they can be integrated to
take advantage of cross-media to clarify the video contents
and remove the redundant information. The text documents for
the closed captions may not synchronize with the video well
and generally have a delay of a few seconds. Nevertheless,
the audio generally synchronizes very well with the video but
the accuracy of most existing techniques for automatic speech
recognition (ASR) is still low. By integrating the results for
automatic speech recognition with the results of closed caption
analysis, the closed captions can be synchronized with the video
contents with higher accuracy.
After the closed captions are synchronized to the relevant
news videos, we can determine the correlation between the
closed captions and the video shots. To do this, the closed
captions are first segmented to sentences, and the start time
and the stop time for each text sentence can also be obtained
automatically. All these video shots that locate between the
start time and the stop time for the same text sentence are
associated with the corresponding text sentence. In addition, the
text sentence is further segmented into keywords, as discussed
8
in Section 3. Finally, all the video shots are associated with
the relevant keywords in the same text sentence. After all shots
have been associated with keywords, the keyword weight of a
shot is computed by:
wkeyword (i) = max
x
{wt (x) |x is a keyword of i} (15)
where w¯t (•) is the news topics weighting algorithm of Eq. (7),
which is introduced in Section 4. The reason to use max {•} to
compute the overall keyword weight is that almost every shot is
associated with several high-frequent uninteresting keywords,
and really interesting keywords are generally rare. If any av-
erage algorithm is used, the calculated value is dominated by
the low weights of high-frequent uninteresting keywords. The
max {•} operation ensures our algorithm to detect the exis-
tence of really interesting keywords robustly.
With the keyword weight and the visual weight computed
above, the overall weight for a given video shot is determined
by averaging wvideo and wkeyword:
w (i) = ? × wvideo (i) + (1? ?)× wkeyword (i) (16)
In our current experiments, we set ? = 0.6.
6. Visualization for Large-scale News Video Exploration
After the news topics and semantic items are available, we
introduce our visualization framework to enable more effective
retrieval and exploration of large-scale news video collections.
Because the available scene size for news video visualization is
limited and there are large amount of interesting news topics, we
need to answer three questions: (a) Which news topics are more
interesting to users and should receive more space for display?
(b) How can we display large amount of interesting news topics
and their associations more effectively on a limited-size scene?
(c) Which visualization method is the most suitable? (d) How
can we visualize the changing trend of news topics along the
time?
6.1. Visualization of Relation Network
As the news reports are unpredictable, both the general au-
diences and the news analysts may first want to have a global
overview of all the available news reports. This means that the
global overview is the first piece of information that is mean-
ingful for most audiences, and such global overview is not nec-
essarily formed by the whole news stories. It can be composed
of the news topics with their interestingness weights and their
associations. For providing the global overview of large-scale
news video collections, the news topics and their associations
are better than the whole news stories because the audiences
may be interested in a certain small piece of information of a
news report, such as a name or an interesting video shot.
The news topics can provide a good hint to the users, and thus
the users can quickly make the decision of which news topic
is more interesting than others according to their own prefer-
ences. By acquainting the users with a better global overview
of large-scale news video collections, they can easily and in-
tuitively specify their queries by clicking the relevant news
topics interactively, and our system will return the most rele-
vant news video clips which are strongly related to the corre-
sponding news topics. Compared with the traditional keyword-
based news video retrieval systems, our system can visually
acquaint the users with: (a) keywords for news topic interpre-
tation; (b) associations between the news topics; (c) interest-
ingness weights to suppress the less interesting news topics.
Because the less interesting news topics are suppressed and the
unexpected news topics are emphasized, our proposed visual-
ization algorithm can allow users to find the most interesting
information easily. On the other hand, the associations among
the news topics can also disclose interesting and meaningful
information to the users.
For naive users to harvest our research achievements, it is
very important to develop more comprehensive framework for
visualizing such relation network of news topics, so that they
can specify their queries easily and intuitively or explore large-
scale news video collections interactively. Visualizing large-
scale relation network in two-dimensional system interface is
not a trivial task [26, 32, 34, 37, 43, 48]. We have developed a
framework integrating multiple innovative ideas to tackle this
issue effectively: (a) A tree-based approach is incorporated to
visualize the relation network in a nested graph view, where
each news topic is displayed along with its relevant ones. (b)
The geometric closeness of the news topics on the visualization
tree is related to their semantic relevance and associations, so
that our graphical presentation can reveal a great deal about
how these news topics are organized and how they are intended
to be used. (c) Both geometric zooming and semantic zooming
are integrated to adjust the level of visible detail automatically
according to the discerning constraint on the number of news
topics that can be displayed per visualization view.
Our approach for relation network visualization exploits hy-
perbolic geometry [26]. The hyperbolic geometry is particu-
larly well suited to graph-based layout of large-scale relation
network because of two reasons. First, the area of a circle on
a hyperbolic plane rises exponentially in radius. Because the
number of nodes of the relation network increases exponen-
tially as the depth increases, these nodes can be put on a hyper-
bolic plane in equal density. Second, the hyperbolic plane can
be projected to the Euclidean plane by Poincaré disk model [26]
to implement a effect similar to the log-polar transformation in
a uniform framework. This effect is essential to visualization
because it matches the property of the human vision [6].
The essence of our approach is to project the relation net-
work onto a hyperbolic plane according to the contextual rela-
tionships between the news topics, and layout the relation net-
work by mapping the relevant news topics onto a circular dis-
play region. Thus our relation network visualization framework
takes the following steps: (a) The news topics on the relation
network are projected to a hyperbolic plane according to their
associations, and such projection can usually preserve the orig-
inal associations between the news topics. (b) After we obtain
such context-preserving projection of the news topics, we can
then use Poincaré disk model [26] to map the news topics on
9
the hyperbolic plane to a 2D display coordinate. Poincaré disk
model maps the entire hyperbolic space onto an open unit cir-
cle, and produces a non-uniform mapping of the news topics to
the 2D display coordinate. The Poincaré disk model preserves
the angles, but distorts the lines. The Poincaré disk model also
compresses the display space slightly less at the edges, which
in some cases can have the advantage of allowing a better view
of the context around the center of projection.
Our implementation relies on the representation of the hyper-
bolic plane, rigid transformations of the hyperbolic plane and
mappings of the news topics from the hyperbolic plane to the
unit disk. Internally, each news topic on the graph is assigned
a location z = (x, y) within the unit disk, which represents the
Poincaré coordinates of the corresponding news topic. By treat-
ing the location of the news topic as a complex number, we can
define such a mapping as the linear fractional transformation
[26]:
Zt =
?z + P
1 + P?z
(17)
where P and ? are complex numbers, | P |< 1 and | ? |=
1, and P is the complex conjugate of P . This transformation
indicates a rotation by ? around the origin followed by moving
the origin to P (and ?P to the origin).
6.2. Interactive Exploration of Large-Scale News Video
Collections
After the hyperbolic visualization of the relation network is
available, it can be used to enable interactive exploration and
navigation of large-scale news video collections at the topic
level via change of focus. The change of focus is implemented
by changing the mapping of the news topics from the hyper-
bolic plane to the display unit disk. The positions of the news
topics on the hyperbolic plane need not to be altered during fo-
cus manipulation. Users can change their focus of news topics
by clicking on any visible news topic to bring it into focus at
the center, or by dragging any visible news topic interactively
to any other location without losing the contextual relation-
ships between the news topics, where the rest of the layout of
the relation network transforms appropriately. Thus our hyper-
bolic framework for relation network visualization has demon-
strated the remarkable capabilities for interactively exploring
large-scale news video collections. By supporting change of fo-
cus, our hyperbolic visualization framework can theoretically
display unlimited number of news topics in a 2D unit disk.
Moving the focus point over the display disk unit is equiva-
lent to translating the relation network on the hyperbolic plane,
such change of focus can provide a mechanism for control-
ling which portion of the relation network receives the most
space. Through such change of focus on the display disk unit
for relation network visualization and manipulation, the users
are able to interactively explore and navigate large-scale news
video archives. Therefore, users can always see the details of
the regions of interest by changing the focus. Different views of
the layout results of our relation network are given in Figures
Fig. 5. Display emphasizing the top-left part.
Fig. 6. Display emphasizing the top-right part.
5-8. By changing the focus points, our hyperbolic framework
for relation network visualization can provide an effective so-
lution for interactive exploration of large-scale news video col-
lections at the topic level. The users can rotate, translate and
zoom the network to examine the details at different level. An
online demo can be found at http://webpages.uncc.edu/~hluo/
relation/Relation.html. The relations can disclose another level
of knowledge to the users.
6.3. Intuitive Query Specification
Our hyperbolic visualization of the relation network can ac-
quaint the users with a good global view of the overall informa-
tion of large-scale news video collections at the first glance, so
that users can specify their queries visually because the relevant
keywords for news topic interpretation and the most representa-
10
Fig. 7. Display emphasizing the bottom-left part.
Fig. 8. Display emphasizing the bottom-right part.
tive keyframes are visible. In addition to such specific queries,
our framework can allow users to start at any level of the rela-
tion network and navigate towards more details by clicking the
relevant news topics of interest to change the focus. Therefore,
our graphical visualization framework can significantly extend
user’s ability on video access and allow users to explore large-
scale news video collections interactively at different levels of
details. After the users click the relevant news topics, our sys-
tem can then retrieve the news video databases according to
the selected news topic and most relevant news stories are se-
lected and returned to the users. The retrieved stories can be
organized by timeline so that the users can easily learn the his-
tory of the whole event, as shown in Figure 9(a). In addition,
the most relevant web news can also be retrieved, as shown in
9(b). This feature is very important for audiences who want to
know more details and relevant discussions of the event, and
(a) Results by timeline
(b) Cross media retrieval results
Fig. 9. An example of search results.
thus our system can provide a good technique of reasoning.
6.4. User-Adaptive Visualization of Relation Network
After the users navigate the relation network for large-scale
news video collections, they can specify their queries according
to their interestingness. Therefore, the users can obtain more
relevant news stories by submitting some specific queries. Con-
sequently, we must extract a new news topic relation network
that is relevant to the user input but still preserves the global
semantic context. To achieve this purpose, we “boost” the rel-
evant knowledge items relevant to the user input on the global
network by a relevance factor:
K´D (sI) = {(ki, wU (ki)×$ (ki, sI)) | 1 ? i ? N}
=
{(
ki, wU (ki)× cmax{r(sa,sI),r(sb,sI)}
)} (18)
where sI is the clicked item, $ (ki, sI) is a boosting factor
to emphasize items most relevant to sI , c ? 1 is the boost-
11
ing constant, and r (s?, sI) ? [0, 1] is the relevance between
s? and sI . By applying Eq. (18) to the global network, irrele-
vant knowledge items with r (s?, sI) = 0 stay unchanged and
relevant knowledge items with r (s?, sI) > 0 have interesting-
ness weights increased according to their relevance to the user
input. As a result, more relevant knowledge items are selected
for visualization on the new network. Constant c balances the
local details and the global context. Larger c enables more local
details to be included in the new network. Smaller c preserves
more global context in the new network.
To compute K´D (sI), r (s?, sI) must be computed. Because
the news topic relation network K´D represents the relevance
quantities among news topics, r (sm, sn) can be computed by
exploring KD. Between a pair of news topics sm and sn, there
may be several paths px (sm, sn) = (sm, ..., sl, ..., sn) on KD.
The interestingness of px (sm, sn) is defined as:
wU (px (sm, sn)) = min {wU (kj = (sa, sb))} (19)
where kj is a segment of px (sm, sn). The shortest path is
defined as:
pmin (sm, sn) = arg max
x
{wU (px (sm, sn))} (20)
The shortest path pmin (sm, sn) represents the most interesting
route connecting sm and sn. Therefore, it is a good measure
of the relevance between sm and sn:
r (sm, sn) = wU (pmin (sm, sn)) (21)
By combining Eq. (21) into Eq. (18), a new semantic network
K´D (sI) can be generated, which is relevant to the user input
sI . In addition, relevant nodes are automatically laid out close
to sI . Relevant events can then be easily checked. Furthermore,
the global semantic context is still preserved, so that the user
can quickly switch to new point of interest if she changes her
mind. Examples are given in Figures 10-11. Such user-adaptive
visualization of relation network can provide the users more
details of the relevant news.
6.5. News Changing Trend Visualization
As the content of news reports is dynamic (i.e., news are
changed along the time), the audiences may also want to see the
news changing trend over time. Such dynamic news trend is
able to tell the history of the whole news event to the users, and
thus the users can have more complete view of the interesting
topics with the dynamic trend.
To effectively depict the above information, several visual-
ization techniques are adopted. Firstly, global overview infor-
mation is represented by using the keyframes map, as shown in
Figure 12(a) and 12(d). The size of the keyframe in the map is
proportional to the interestingness weight of the relevant news
topic. By organizing the global overview information in this
way, the users can directly find the interesting news reports on
the keyframes map and learn the global structure of all news
reports. To represent the dynamic trend of the news reports, an
animation of keyframes on the keyframes map is used. Two an-
imation frames are given in 12(b) and 12(c). By watching the
(a) “Robert Gates” in global relation network
(b) Detailed view focusing on “Robert Gates”
Fig. 10. Adaptation example for query “Robert Gates”.
animation the users are able to catch the dynamic trend of the
news topics.
7. Experiments
To evaluate the efficiency of our system, we compare our sys-
tem with the state of the art news search engine, Google News.
We ask users to evaluate the difficulty of answering several
news related questions by using our system and Google News.
12
(a) “education” in global relation network
(b) Detailed view focusing on “education”
Fig. 11. Adaptation example for query “education”.
Total 12 users participated the experiments. 10 of them are un-
dergraduate students without any related background, and 2 of
them are security experts. Half of the users evaluate our system
first. Another half evaluate Google News first. Before a user
evaluate our system, the user watches a two-minute introduc-
tion video. For each task, the users give out only the difficulty
level to complete the task. The difficulty level is defined as a
number between 1 and 10, where 1 is the lowest level and 10
is the highest level. The database used in the evaluation con-
(a) (b)
(c) (d)
Fig. 12. An example of video news visualization. (a) Keyframes map
for U.S. news on July 22, 2006; (b) and (c) Intermediate animation; (d)
Keyframes map for U.S. news on July 23, 2006. The keyframes maps
show the news topics on given day, the animation represents the trend of
topic change over time. An example of animation can be downloaded at
http://webpages.uncc.edu/~hluo/NewsDemo.avi.
tains three channels (CNN, FOX, and MSNBC) of video news
reports in the past month (which is October, 2006).
The first task is to list several most important news events in
the past month. The average difficulty level for Google News is
9.2, and that for our system is 4.5. Most users said Google News
provides little help on completing this task. The two security
experts said our system is very helpful to complete this task,
and this task is typical for their everyday work.
The second task is to summarize the whole event of North
Korean nuclear weapon test. The average difficulty level for
Google News is 6.6, and that for our system is 4.3. The users
said that our system places relevant news topics immediately
surrounding the point of focus, which is very helpful to figure
out the rough aspects of the whole event.
The third task is to answer when, where, why and how of the
Amish school shooting. The average difficulty level for Google
News is 4.1, and that for our system is 6.7. Google News out-
performs our system in this task. The most two important rea-
sons given by the users are: (1) The keyword-based search tech-
nique of Google News is significantly better than ours; (2) It is
much easier to extract fine details from the web news reports
than from the video news reports.
At February 2008, we perform a new experiment for January
2008 database. During this experiment, the average completion
13
time is used as the evaluation criteria. Total 20 undergraduate
students participate the experiment. 10 of them are asked to use
Google News and another 10 are asked to use our system. A
participant only use one of the two systems. None of them has
used our system before.
The first task is to find an interesting news and read it. The
average completion time for Google News is 0.9 minutes, and
that for our system is 0.5 minutes. We asked the participants
to press the “complete” button only when they read a really
interesting news. Therefore, a user may read more than one
news to complete this task. As a result, the average completion
time is longer than the time to perform the first click.
The second task is to list 5 most important news events in the
past month. The average completion time for Google News is
2.3 minutes, and that for our system is 5.5 minutes. The result
is completely different than our expectation. After checking
the answer sheets and inquiring the participants, we found that
the users of Google News answered this question primarily by
memory, while the users of our system did actually explore the
database and check the news reports carefully. As a result, 4 of
the Google News users listed less than 5 news reports, while
all the users of our system listed 5 news reports.
The third task is to summarize the first event listed in the
second task. The average completion time for Google News is
3.7 minutes, and that for our system is 3.3 minutes. Our sys-
tem outperforms Google News slight. The difference is less
than what we expected. We again inquired the participant and
checked the experiment log, found that watching the video re-
ports consumes more time than reading text reports for summa-
rization purpose. Therefore, even though our system can help
the users find relevant reports in shorter time, the users need to
use more time to watch the reports. If we exclude the time for
reading or watching by using the experiment log, the average
completion time becomes 1.5 minutes for Google News users
v.s. 0.8 minutes for users of our system.
Based on the above experiments, one can find that: (1) Our
system provides valuable service when the users do not have
detailed preference. (2) Sophisticated keyword-based search
techniques perform better when the users have detailed prefer-
ence and need to learn the fine details. Therefore, our system
is able to guide the users to build their own preference effec-
tively and efficiently. Then keyword-based search techniques
can be adopted to disclose fine details after the system catches
the user’s fine preference.
8. Conclusions
In this paper, we have developed a novel framework to
achieve more effective analysis, retrieval and exploration of
large-scale news video collections. By integrating cross-media
information from multiple sources and synchronizing multi-
modal content analysis results, our proposed schemes can
achieve more effective news topic detection and interesting-
ness assignment and bridge the semantic gap successfully.
By incorporating hyperbolic visualization for relation network
visualization, our system can also support more effective re-
trieval and exploration of large-scale news video collections.
Our experiments on large-scale news video collections have
provided very positive results.
9. Acknowledgment
This work was sponsored by the National Visualization and
Analytics Center (NVACTM) under the auspices of the South-
east Regional Visualization and Analytics Center. NVAC is a
U.S. Department of Homeland Security Program led by Pacific
Northwest National Laboratory.
References
[1] Brett Adams, Chitra Dorai, and Svetha Venkatesh. To-
wards automatic extraction of expressive elements from
motion pictures:tempo. IEEE Trans. on Multimedia,
4(4):472–481, 2002.
[2] W. H. Adams, Giridharan Iyengar, Chingyung Lin, Milind
Naphade, Chalapathy Neti, Herriet Nock, and John R.
Smith. Semantic indexing of multimedia content using
visual, audio and text cues. EURASIP Journal on Applied
Signal Processing, 2003(2):170–185, 2003.
[3] G. Antini, S. Berretti, A. Del Bimbo, and P. Pala. 3d face
identification based on arrangement of salient wrinkles.
In Proceedings of the IEEE International Conference on
Multimedia and Expo (ICME 2006), pages 85–88, 2006.
[4] Thanos Athanasiadis, Phivos Mylonas, Yannis Avrithis,
and Stefanos Kollias. Semantic image segmentation and
object labeling. IEEE Trans. On CSVT, 17(3):298–312,
2007.
[5] K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth, D. Blei,
and M.I. Jordan. Matching words and pictures. Journal
of Machine Learning Research, 3:1107–1135, 2003.
[6] Alexandre Bernardino and José Santos-Victor. Binocu-
lar visual tracking: Integration of perception and control.
IEEE Transactions on Robotics and Automation, 6:15,
1999.
[7] H. Le Borgne, A., Guerin-Dugue, and N.E. O’Connor.
Learning midlevel image features for natural scene and
texture classification. IEEE Trans. On CSVT, 17(3):286–
297, 2007.
[8] H. Bruce, B. Cleal, R. Fidel, and A.M.Pejtersen. A multi-
dimensional approach to the study of human-information
interaction: a case study of collaborative information re-
trieval. Journal of the American Society for Information
Science and Technology, 55(11):939–953, 2004.
[9] D. Bulgarelli, C. Grana, R. Vezzani, and R. Cucchiara. A
semi-automatic video annotation tool with mpeg-7 content
collections. In Proceedings of IEEE International Sym-
posium on Multimedia (ISM2006), pages 742–745, 2006.
[10] Yixin Chen and James Z. Wang. Image categorization by
learning and reasoning with regions. Journal of Machine
Learning Research, 5:913–939, 2004.
14
[11] S. Christodoulakis and C. Tsinaraki. A multimedia user
preference model that supports semantics and its appli-
cation to mpeg 7/21. In Proceedings of the Multimedia
Modeling 2006 Conference (MMM 2006), pages 35–42,
2006.
[12] Saman Cooray, Noel O’Connor, Sean Marlow, Noel Mur-
phy, and Thomas Curran. Semi-automatic video object
segmentation using recursive shortest spanning tree and
binary partition tree. In Workshop on Image Analysis For
Multimedia Interactive Services, 2001.
[13] S. Dasiopoulou, C. Doulaverakis, V. Mezaris, I. Kompat-
siaris, and M.G. Strintzis. Semantic-Based Visual Infor-
mation Retrieval, chapter An Ontology-Based Framework
for Semantic Image Analysis and Retrieval. Idea Group
Inc., 2007.
[14] S. Dasiopoulou, V. Mezaris, I. Kompatsiaris, V. K. Pa-
pastathis, and M. G. Strintzis. Knowledge-assisted se-
mantic video object detection. IEEE Trans. On CSVT,
15(10):1210–1224, 2005.
[15] S. Dasiopoulou, C. Saathoff, Ph. Mylonas, Y. Avrithis,
Y. Kompatsiaris, S. Staab, and M.G. Strintzis. Seman-
tic Multimedia and Ontologies: Theory and Applications,
chapter Introducing Context and Reasoning in Visual Con-
tent Analysis: An Ontology-based Framework. Springer-
Verlang, 2007.
[16] Jianping Fan, Yuli Gao, and Hangzai Luo. Multi-level an-
notation of natural scenes using dominant image compo-
nents and semantic image concepts. In ACM Multimedia,
pages 540–547, 2004.
[17] Jianping Fan, Hangzai Luo, and Ahmed K. Elmagarmid.
Concept-oriented indexing of video database toward more
effective retrieval and browsing. IEEE Trans. on Image
Processing, 13(7):974–992, 2004.
[18] Julien Fauqueur and Nozha Boujemaa. Region-based im-
age retrieval: Fast coarse segmentation and fine color de-
scription. Journal of Visual Languages and Computing,
15(1):69–95, 2004.
[19] Kingshy Goh, Beitao Li, and Edward Y. Chang. Seman-
tics and feature discovery via confidence-based ensemble.
ACM Trans. on Multimedia Computing, Communications,
and Applications, 1(2):168 – 189, 2005.
[20] Alex Hauptmann and Michael Smith. Text, speech, and
vision for video segmentation: The informedia project. In
AAAI Fall 1995 Symposium on Computational Models for
Integrating Language and V, 1995.
[21] Alexander G. Hauptmann. Lessons for the future from a
decade of informedia video analysis research. In Interna-
tional Conference on Image and Video Retrieval (CIVR),
volume LNCS 3568, pages 1–10, 2005.
[22] Susan Havre, Beth Hetzler, and Lucy Nowell. Themeriver:
Visualizing theme changes over time. In IEEE Symposium
on Information Visualization (InfoVis), pages 115–123,
2000.
[23] Elizabeth G. Hetzler, Paul Whitney, Lou Martucci, and
Jim Thomas. Multi-faceted insight through interoperable
visual information analysis paradigms. In IEEE Sympo-
sium on Information Visualization, page 137, 1998.
[24] Derek Hoiem, Rahul Sukthankar, Henry Schneiderman,
and Larry Huston. Object-based image retrieval using the
statistical structure of images. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages
490–497, 2004.
[25] Alias i Inc. Lingpipe. http://www.alias-i.com/lingpipe/.
[26] John Lamping and Ramana Rao. The hyperbolic browser:
A focus+context technique based on hyperbolic geometry
for visualizing large hierarchies. Journal of Visual Lan-
guages and Computing, 7(1):33–55, 1996.
[27] Wei-Hao Lin and Alexander Hauptmann. News video
classification using svm-based multimodal classifiers and
combination strategies. In ACM Multimedia, 2002.
[28] Tatiana Louchnikova and Stephane Marchand-Maillet.
Flexible image decomposition for multimedia indexing
and retrieval. In SPIE Internet Imaging, pages 203–211,
2002.
[29] Hangzai Luo, Jianping Fan, Jin Yang, William Ribarsky,
and Shin’ichi Satoh. Exploring large-scale video news via
interactive visualization. In IEEE Symposium on Visual
Analytics Science and Technology, pages 75–82, 2006.
[30] J. Maydt and R. Lienhart. An extended set of haar-like
features for rapid object detection. In Proceedings of
the International Conference on Image Processing (ICIP
2002), volume 1, pages 900–903, 2002.
[31] Andrew Mehler, Yunfan Bao, Xin Li, Yue Wang, and
Steven Skiena. Spatial analysis of news sources.
IEEE Trans. on Visualization and Computer Graphics,
12(5):765–772, 2006.
[32] Baback Moghaddam, Qi Tian, Neal Lesh, Chia Shen, and
Thomas S. Huang. Visualization & user-modeling for
browsing personal photo libraries. International Journal
of Computer Vision, 56:109 – 130, 2004.
[33] Milind Ramesh Naphade, Igor V. Kozintsev, and
Thomas S. Huang. Factor graph framework for semantic
video indexing. IEEE Trans. on CSVT, 12:40–52, 2002.
[34] G.P. Nguyen and M.Worring. Similarity based visualiza-
tion of image collections. In AVIVDiLib’05, 2005.
[35] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. The pagerank citation ranking: Bringing order
to the web. http://dbpubs.stanford.edu:8090/pub/1999-66.
[36] Yossi Rubner and Carlo Tomasi. Texture-based image re-
trieval without segmentation. In IEEE International Con-
ference on Computer Vision (ICCV), pages 1018–1024,
1999.
[37] Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas.
A metric for distributions with applications to image
databases. In IEEE ICCV, 1998.
[38] Shin’ichi Satoh and Norio Katayama. An efficient imple-
mentation and evaluation of robust face sequence match-
ing. In International Conference on Image Analysis and
Processing, pages 266–271, 1999.
[39] Helmut Schmid. Probabilistic part-of-speech tagging us-
ing decision trees. In International Conference on New
Methods in Language Processing, Manchester, UK, 1994.
[40] Arnold W.M. Smeulders, Marcel Worring, Simone San-
tini, Amarnath Gupta, and Ramesh Jain. Content-base im-
15
age retrieval at the end of the early years. IEEE Trans. on
Pattern Analysis and Machine Intelligence, 22(12):1349–
1380, December 2000.
[41] Cees G. M. Snoek, Marcel Worring, and Alexander G.
Hauptmann. Learning rich semantics from news video
archives by style analysis. ACM Trans. on Multime-
dia Computing, Communications, and Applications, 2:91–
108, 2006.
[42] Cees G.M. Snoek, Marcel Worring, Jan-Mark Geuse-
broek, Dennis C. Koelma, Frank J. Seinstra, and
Arnold W.M. Smeulders. The semantic pathfinder: Using
an authoring metaphor for generic multimedia indexing.
IEEE Trans. on PAMI, 28:1678–1689, 2006.
[43] Daniela Stan and Ishwar K. Sethi. eid: a system for ex-
ploration of image databases. Information Processing and
Management, 39:335 – 361, 2003.
[44] G. Sudhir, John C. M. Lee, and Anil K. Jain. Automatic
classification of tennis video for high-level content-based
retrieval. In CAIVD ’98, 1998.
[45] Russell Swan and David Jensen. Timemines: Construct-
ing timelines with statistical models of word. In ACM
SIGKDD, pages 73–80, 2000.
[46] David Vallet, Pablo Castells, Miriam Fernandez, Phivos
Mylona, and Yannis Avrithis. Personalized content re-
trieval in context using ontological knowledge. IEEE
Trans. On CSVT, 17(3):336–346, 2007.
[47] Jarke J. van Wijk. Bridging the gaps. Computer Graphics
and Applications, 26(6):6–9, 2006.
[48] Jörg A. Walter and Helge Ritter. On interactive visualiza-
tion of high-dimensional data using the hyperbolic plane.
In ACM SIGKDD, 2002.
[49] Marcos Weskamp. Newsmap. http://www.marumushi.
com/apps/newsmap/index.cfm.
[50] James A. Wise, James J. Thomas, Kelly Pennock, David
Lantrip, Marc Pottier, Anne Schur, and Vern Crow. Vi-
sualizing the non-visual: Spatial analysis and interaction
with information from text documents. In IEEE Sympo-
sium on Information Visualization (InfoVis), pages 51–58,
1995.
[51] Yi Wu, Edward Y. Chang, Kevin Chen-Chuan Chang, and
John R. Smith. Optimal multimodal fusion for multimedia
data analysis. In ACM Multimedia, 2004.
[52] Wensheng Zhou, Asha Vellaikal, and C. C. Jay Kuo. Rule-
based video classification system for basketball video in-
dexing. In ACM Multimedia, 2000.
16

